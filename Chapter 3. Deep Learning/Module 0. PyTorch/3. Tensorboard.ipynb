{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard\n",
    "\n",
    "Tensorboard is a tool that we can use to visualise all sorts of things about our models. It was originally intended to be used with Google's Tensorflow framework but PyTorch now supports it since version 1.2.0.\n",
    "\n",
    "## Running tensorboard in the notebook (not recommended)\n",
    "\n",
    "If you want to visualise this stuff in a notebook, just run the \"magic\" commands shown below in a code cell in your notebook.\n",
    "The % sign here is not valid Python syntax and as such will not work outside of a notebook. The availability of the commands depends on your IPython kernel.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n      <iframe id=\"tensorboard-frame-35c4cafac90c287a\" width=\"100%\" height=\"800\" frameborder=\"0\">\n      </iframe>\n      <script>\n        (function() {\n          const frame = document.getElementById(\"tensorboard-frame-35c4cafac90c287a\");\n          const url = new URL(\"/\", window.location);\n          url.port = 6006;\n          frame.src = url;\n        })();\n      </script>\n  "
     },
     "metadata": {}
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "source": [
    "## Running tensorboard outside of a notebook\n",
    "\n",
    "First you need to install it through pip by running `pip3 install tensorboard`\n",
    "\n",
    "Next, navigate to the directory which your notebook file is in a terminal then run `tensorboard --logdir runs`. It should prompt you to open http://localhost:6006/ in a browser. When you do that, you'll be able to see tensorboard running there.\n",
    "\n",
    "The ```--logdir runs``` tag tells tensorboard that you want the logs to be stored in a directory (hence logdir) called runs (you can store them anywhere else too).\n",
    "\n",
    "Firstly, there will be no data here. During the training loop, we will populate the logging directory."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Tags visualise the data in different places on the GUI. For instance we might tag different curves for training and testing to visualise them in different graphs.\n",
    "\n",
    "Runs group together data that was produced by the same writer. To separate out different runs, specify a new folder for the writer to store the data for that run in. We do this by specifying that filepath as an argument when we initilise the SummaryWriter."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import get_dataloaders, NN\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "section = 'tensorboard_test'\n",
    "graph_name = 'loss'\n",
    "\n",
    "train_loader, val_loader, test_loader = get_dataloaders()\n",
    "\n",
    "# TRAINING LOOP\n",
    "def train(model, epochs=1):\n",
    "\n",
    "    optimiser = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    # CREATE AN OBJECT WHICH CAN WRITE TO TENSORBOARD\n",
    "    run_id = time.time() # use the timestamp as a unique id for this run\n",
    "    writer = SummaryWriter(log_dir=f'../../runs/{section}/{run_id}')\n",
    "    batch_idx = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch in train_loader:\n",
    "\n",
    "            # USUAL STUFF\n",
    "            x, y = batch\n",
    "            prediction = model(x)\n",
    "            loss = F.cross_entropy(prediction, y)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "            # WRITE THE LOSS TO TENSORBOARD\n",
    "            writer.add_scalar(f'{section}/{graph_name}', loss.item(), batch_idx)\n",
    "            batch_idx += 1\n",
    "\n",
    "my_model = NN([784, 32, 10], distribution=True, flatten_input=True) # create a neural network. More on this later\n",
    "train(my_model)"
   ]
  },
  {
   "source": [
    "Once you run the above cell, you should be able to see the graph, in the specified section, in tensorboard.\n",
    "\n",
    "Pay careful attention to how the data is displayed in each graph, in different sections, and how they are uniquely referenced."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('main': conda)",
   "display_name": "Python 3.8.5 64-bit ('main': conda)",
   "metadata": {
    "interpreter": {
     "hash": "06c1e258a470a687113bfba03f207c092b27379067ada2d83b8b31269ab641fe"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}