{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('main': conda)",
   "display_name": "Python 3.8.5 64-bit ('main': conda)",
   "metadata": {
    "interpreter": {
     "hash": "06c1e258a470a687113bfba03f207c092b27379067ada2d83b8b31269ab641fe"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Optimisation for Deep Learning\n",
    "\n",
    "Learning outcomes\n",
    "- understand mathematically and intuitively the most common optimisation algorithms used for optimising deep models\n",
    "- implement your own optimiser in PyTorch\n",
    "\n",
    "## Reminder of gradient based optimisation\n",
    "\n",
    "So far we've looked at some pretty simple gradient based optimisers including gradient descent and stochastic gradient descent.\n",
    "\n",
    "In this notebook, we'll look at some more complex optimisers, which can overcome some of the shortcomings of the methods we've looked at previously, and will allow us to train deep models more quickly.\n",
    "\n",
    "Here's a visualisation of how different optimisers might iteratively update the model weights.\n",
    "\n",
    "![](images/optim_vis.gif)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Challenges with optimising deep models\n",
    "- Local structure may not be representative of global structure\n",
    "- \n",
    "\n",
    "## Again, don't be scared by local optima\n",
    "\n",
    "1. Local minima become exponentially rare with the number of parameters in the model\n",
    "For every weight, we will compute how the loss changes with respect to it.\n",
    "It becomes exponentially unlikely that the rate of change along every weight axis will be positive. \n",
    "\n",
    "2. Empirically, local minima perform well enough!\n",
    "Even local minima can achieve "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Gradient Descent\n",
    "\n",
    "![](images/gradient_descent.jpg)\n",
    "\n",
    "## SGD\n",
    "\n",
    "![](images/SGD.jpg)\n",
    "\n",
    "## SGD with momentum\n",
    "\n",
    "![](images/momentum.jpg)\n",
    "\n",
    "## SGD with Nesterov momentum\n",
    "\n",
    "![](images/nesterov.jpg)\n",
    "\n",
    "## AdaGrad\n",
    "\n",
    "Is there a more systematic way to reduce the learning rate over time?\n",
    "\n",
    "AdaGrad assumes so, and reduces the learning rate\n",
    "\n",
    "![](images/adagrad.jpg)\n",
    "\n",
    "## RMSProp\n",
    "\n",
    "The problem with AdaGrad is that the learning rate can never recover and increase to speed up optimisation once it has slowed down, it can only decrease further. So if a steep part of the loss surface is encountered before a flatter part, the learning rate for this parameter will be divided by the large loss surface gradient in the steep region and be too small to make meaningful progress in the flatter region.\n",
    "\n",
    "RMSProp is similar to AdaGrad except for how it accumulates the gradient to decay the learning rate for each parameter. Instead of continuuing to sum up the square of all of the gradients encountered in each given direction, it takes an *exponential moving average*. This gives the chance for the learning rate to increase if a steep gradient were not encountered recently, as the historical gradients encountered have an exponentially smaller influence on the learning rate with each optimisation step.\n",
    "\n",
    "![](images/rmsprop.jpg)\n",
    "\n",
    "## Adam\n",
    "\n",
    "![](images/adam.jpg)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## So which algorithm do I use?\n",
    "\n",
    "Well... as usual, it depends on your problem and your dataset.\n",
    "\n",
    "It's still a highly active field of research. But in general, **SGD with momentum or Adam** are the go to choices for optimising deep models."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Using these optimisation algorithms\n",
    "\n",
    "Let's set up the same neural network as in the previous module, and then switch out the optimiser for Adam and others and show how you can adapt it to use momentum."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import NN, get_dataloaders\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "my_nn = NN([784, 1024, 1024, 512, 10], distribution=True, flatten_input=True)\n",
    "\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# HOW TO USE DIFFERENT OPTIMISERS PROVIDED BY PYTORCH\n",
    "optimiser = torch.optim.SGD(my_nn.parameters(), lr=learning_rate, momentum=0.1)\n",
    "# optimiser = torch.optim.Adagrad(NN.parameters(), lr=learning_rate)\n",
    "# optimiser = torch.optim.RMSprop(NN.parameters(), lr=learning_rate)\n",
    "optimiser = torch.optim.Adam(my_nn.parameters(), lr=learning_rate)"
   ]
  },
  {
   "source": [
    "The stuff below is exactly the same as before!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GET DATALOADERS\n",
    "test_loader, val_loader, train_loader = get_dataloaders()\n",
    "criterion = F.cross_entropy\n",
    "\n",
    "# SET UP TRAINING VISUALISATION\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# TRAINING LOOP\n",
    "def train(model, optimiser, graph_name, epochs=1, tag='Loss/Train'):\n",
    "    writer = SummaryWriter(log_dir=f'../../runs/{tag}') # make a different writer for each tagged optimisation run\n",
    "    for epoch in range(epochs):\n",
    "        for idx, minibatch in enumerate(train_loader):\n",
    "            inputs, labels = minibatch\n",
    "            prediction = model(inputs)             # pass the data forward through the model\n",
    "            loss = criterion(prediction, labels)   # compute the loss\n",
    "            print('Epoch:', epoch, '\\tBatch:', idx, '\\tLoss:', loss)\n",
    "            optimiser.zero_grad()                  # reset the gradients attribute of each of the model's params to zero\n",
    "            loss.backward()                        # backward pass to compute and set all of the model param's gradients\n",
    "            optimiser.step()                       # update the model's parameters\n",
    "            writer.add_scalar(f'Loss/{graph_name}', loss, epoch*len(train_loader) + idx)    # write loss to a graph\n",
    "\n",
    "# train(my_nn, optimiser)"
   ]
  },
  {
   "source": [
    "Let's compare the training curves generated using some of the optimisers that we explained above."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " \tLoss: tensor(2.0922, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 340 \tLoss: tensor(2.2231, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 341 \tLoss: tensor(2.1283, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 342 \tLoss: tensor(2.1725, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 343 \tLoss: tensor(2.1062, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 344 \tLoss: tensor(2.2220, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 345 \tLoss: tensor(2.1263, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 346 \tLoss: tensor(2.0094, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 347 \tLoss: tensor(2.1392, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 348 \tLoss: tensor(2.0826, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 349 \tLoss: tensor(2.1476, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 350 \tLoss: tensor(1.9845, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 351 \tLoss: tensor(2.1309, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 352 \tLoss: tensor(2.1356, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 353 \tLoss: tensor(2.1836, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 354 \tLoss: tensor(2.1755, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 355 \tLoss: tensor(2.1759, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 356 \tLoss: tensor(2.1185, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 357 \tLoss: tensor(2.2270, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 358 \tLoss: tensor(2.2022, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 359 \tLoss: tensor(2.1471, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 360 \tLoss: tensor(2.1814, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 361 \tLoss: tensor(2.1836, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 362 \tLoss: tensor(2.0657, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 363 \tLoss: tensor(2.1425, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 364 \tLoss: tensor(2.1760, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 365 \tLoss: tensor(2.1230, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 366 \tLoss: tensor(2.1277, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 367 \tLoss: tensor(2.1399, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 368 \tLoss: tensor(2.0514, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 369 \tLoss: tensor(2.1439, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 370 \tLoss: tensor(2.1440, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 371 \tLoss: tensor(2.1954, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 372 \tLoss: tensor(2.1290, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 373 \tLoss: tensor(2.1621, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 374 \tLoss: tensor(2.1643, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 375 \tLoss: tensor(2.1250, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 376 \tLoss: tensor(2.1621, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 377 \tLoss: tensor(2.1207, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 378 \tLoss: tensor(2.1849, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 379 \tLoss: tensor(2.1176, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 380 \tLoss: tensor(2.1167, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 381 \tLoss: tensor(2.0857, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 382 \tLoss: tensor(2.2236, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 383 \tLoss: tensor(2.2045, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 384 \tLoss: tensor(2.1799, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 385 \tLoss: tensor(2.1125, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 386 \tLoss: tensor(2.1478, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 387 \tLoss: tensor(2.1852, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 388 \tLoss: tensor(2.1091, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 389 \tLoss: tensor(2.1219, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 390 \tLoss: tensor(2.0979, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 391 \tLoss: tensor(2.0613, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 392 \tLoss: tensor(2.0832, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 393 \tLoss: tensor(2.0913, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 394 \tLoss: tensor(2.1582, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 395 \tLoss: tensor(2.1008, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 396 \tLoss: tensor(2.0720, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 397 \tLoss: tensor(2.0612, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 398 \tLoss: tensor(2.1853, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 399 \tLoss: tensor(2.1066, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 400 \tLoss: tensor(2.1260, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 401 \tLoss: tensor(2.1823, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 402 \tLoss: tensor(2.0679, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 403 \tLoss: tensor(2.0910, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 404 \tLoss: tensor(2.1169, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 405 \tLoss: tensor(2.0846, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 406 \tLoss: tensor(2.1273, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 407 \tLoss: tensor(2.1786, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 408 \tLoss: tensor(2.0623, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 409 \tLoss: tensor(2.1494, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 410 \tLoss: tensor(2.1014, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 411 \tLoss: tensor(1.9499, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 412 \tLoss: tensor(2.0794, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 413 \tLoss: tensor(2.0696, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 414 \tLoss: tensor(2.0681, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 415 \tLoss: tensor(2.0708, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 416 \tLoss: tensor(2.1241, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 417 \tLoss: tensor(2.1295, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 418 \tLoss: tensor(2.0728, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 419 \tLoss: tensor(2.1432, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 420 \tLoss: tensor(2.0934, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 421 \tLoss: tensor(2.0749, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 422 \tLoss: tensor(2.0640, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 423 \tLoss: tensor(2.1744, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 424 \tLoss: tensor(1.9823, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 425 \tLoss: tensor(2.1247, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 426 \tLoss: tensor(2.0241, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 427 \tLoss: tensor(2.0339, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 428 \tLoss: tensor(2.0869, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 429 \tLoss: tensor(1.9828, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 430 \tLoss: tensor(2.0809, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 431 \tLoss: tensor(2.1392, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 432 \tLoss: tensor(2.0896, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 433 \tLoss: tensor(2.0966, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 434 \tLoss: tensor(2.0065, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 435 \tLoss: tensor(2.0689, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 436 \tLoss: tensor(2.0724, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 437 \tLoss: tensor(2.1028, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 438 \tLoss: tensor(2.0247, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 439 \tLoss: tensor(2.0935, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 440 \tLoss: tensor(2.0905, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 441 \tLoss: tensor(2.1201, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 442 \tLoss: tensor(2.0591, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 443 \tLoss: tensor(2.0108, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 444 \tLoss: tensor(2.0659, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 445 \tLoss: tensor(2.1062, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 446 \tLoss: tensor(2.1476, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 447 \tLoss: tensor(2.1403, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 448 \tLoss: tensor(1.9908, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 449 \tLoss: tensor(2.0226, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 450 \tLoss: tensor(2.0052, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 451 \tLoss: tensor(2.0545, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 452 \tLoss: tensor(2.0763, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 453 \tLoss: tensor(2.1259, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 454 \tLoss: tensor(2.0602, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 455 \tLoss: tensor(2.0626, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 456 \tLoss: tensor(2.1093, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 457 \tLoss: tensor(2.0311, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 458 \tLoss: tensor(2.0351, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 459 \tLoss: tensor(2.0617, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 460 \tLoss: tensor(2.0660, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 461 \tLoss: tensor(2.0913, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 462 \tLoss: tensor(1.9976, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 463 \tLoss: tensor(2.0236, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 464 \tLoss: tensor(2.1047, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 465 \tLoss: tensor(2.1168, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 466 \tLoss: tensor(2.0742, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 467 \tLoss: tensor(2.0954, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 468 \tLoss: tensor(2.0099, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 469 \tLoss: tensor(2.0616, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 470 \tLoss: tensor(1.9524, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 471 \tLoss: tensor(2.0428, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 472 \tLoss: tensor(1.9277, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 473 \tLoss: tensor(2.0034, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 474 \tLoss: tensor(1.9857, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 475 \tLoss: tensor(2.0790, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 476 \tLoss: tensor(2.1108, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 477 \tLoss: tensor(2.0342, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 478 \tLoss: tensor(2.0304, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 479 \tLoss: tensor(2.0188, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 480 \tLoss: tensor(2.0015, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 481 \tLoss: tensor(2.0945, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 482 \tLoss: tensor(2.0898, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 483 \tLoss: tensor(2.0666, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 484 \tLoss: tensor(1.9818, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 485 \tLoss: tensor(2.0434, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 486 \tLoss: tensor(2.0951, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 487 \tLoss: tensor(1.9893, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 488 \tLoss: tensor(2.1184, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 489 \tLoss: tensor(1.8998, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 490 \tLoss: tensor(2.0476, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 491 \tLoss: tensor(2.0518, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 492 \tLoss: tensor(1.9933, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 493 \tLoss: tensor(1.9025, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 494 \tLoss: tensor(1.9783, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 495 \tLoss: tensor(2.0813, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 496 \tLoss: tensor(2.0280, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 497 \tLoss: tensor(2.0848, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 498 \tLoss: tensor(2.0234, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 499 \tLoss: tensor(2.0218, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 500 \tLoss: tensor(2.0612, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 501 \tLoss: tensor(2.0921, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 502 \tLoss: tensor(2.0129, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 503 \tLoss: tensor(1.9408, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 504 \tLoss: tensor(1.9622, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 505 \tLoss: tensor(2.0653, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 506 \tLoss: tensor(1.9541, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 507 \tLoss: tensor(2.0697, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 508 \tLoss: tensor(1.8689, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 509 \tLoss: tensor(1.9641, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 510 \tLoss: tensor(2.0112, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 511 \tLoss: tensor(1.9613, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 512 \tLoss: tensor(2.0085, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 513 \tLoss: tensor(2.0323, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 514 \tLoss: tensor(1.8589, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 515 \tLoss: tensor(1.9551, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 516 \tLoss: tensor(1.9765, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 517 \tLoss: tensor(1.9477, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 518 \tLoss: tensor(2.0265, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 519 \tLoss: tensor(1.9804, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 520 \tLoss: tensor(2.0355, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 521 \tLoss: tensor(1.9753, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 522 \tLoss: tensor(1.9718, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 523 \tLoss: tensor(1.9178, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 524 \tLoss: tensor(1.8852, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 525 \tLoss: tensor(2.0366, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 526 \tLoss: tensor(1.9889, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 527 \tLoss: tensor(1.9406, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 528 \tLoss: tensor(1.9624, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 529 \tLoss: tensor(2.0448, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 530 \tLoss: tensor(2.0016, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 531 \tLoss: tensor(1.9763, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 532 \tLoss: tensor(1.9085, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 533 \tLoss: tensor(1.8765, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 534 \tLoss: tensor(2.0557, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 535 \tLoss: tensor(1.8918, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 536 \tLoss: tensor(1.9906, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 537 \tLoss: tensor(2.0583, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 538 \tLoss: tensor(1.8911, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 539 \tLoss: tensor(2.0020, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 540 \tLoss: tensor(1.9982, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 541 \tLoss: tensor(1.9838, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 542 \tLoss: tensor(2.1610, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 543 \tLoss: tensor(2.0838, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 544 \tLoss: tensor(1.9211, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 545 \tLoss: tensor(1.9921, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 546 \tLoss: tensor(1.9483, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 547 \tLoss: tensor(1.8877, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 548 \tLoss: tensor(1.9608, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 549 \tLoss: tensor(2.0061, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 550 \tLoss: tensor(1.9941, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 551 \tLoss: tensor(1.9765, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 552 \tLoss: tensor(1.9538, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 553 \tLoss: tensor(2.0456, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 554 \tLoss: tensor(1.9667, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 555 \tLoss: tensor(1.9733, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 556 \tLoss: tensor(1.9601, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 557 \tLoss: tensor(2.0724, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 558 \tLoss: tensor(1.9361, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 559 \tLoss: tensor(1.9320, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 560 \tLoss: tensor(2.0383, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 561 \tLoss: tensor(1.8913, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 562 \tLoss: tensor(1.9620, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 563 \tLoss: tensor(1.9097, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 564 \tLoss: tensor(1.9196, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 565 \tLoss: tensor(1.9711, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 566 \tLoss: tensor(2.0149, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 567 \tLoss: tensor(1.8869, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 568 \tLoss: tensor(1.9135, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 569 \tLoss: tensor(1.9577, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 570 \tLoss: tensor(2.0357, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 571 \tLoss: tensor(2.0190, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 572 \tLoss: tensor(2.0719, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 573 \tLoss: tensor(2.0075, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 574 \tLoss: tensor(1.9547, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 575 \tLoss: tensor(1.9803, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 576 \tLoss: tensor(2.0939, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 577 \tLoss: tensor(2.0152, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 578 \tLoss: tensor(2.0113, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 579 \tLoss: tensor(1.9917, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 580 \tLoss: tensor(2.0069, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 581 \tLoss: tensor(1.8765, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 582 \tLoss: tensor(2.0372, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 583 \tLoss: tensor(2.0601, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 584 \tLoss: tensor(1.8909, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 585 \tLoss: tensor(1.9659, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 586 \tLoss: tensor(2.0569, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 587 \tLoss: tensor(2.0635, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 588 \tLoss: tensor(1.9830, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 589 \tLoss: tensor(2.0481, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 590 \tLoss: tensor(1.9952, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 591 \tLoss: tensor(1.9319, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 592 \tLoss: tensor(2.0488, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 593 \tLoss: tensor(2.0471, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 594 \tLoss: tensor(1.9429, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 595 \tLoss: tensor(1.8440, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 596 \tLoss: tensor(1.9706, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 597 \tLoss: tensor(1.9926, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 598 \tLoss: tensor(1.9103, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 599 \tLoss: tensor(1.8952, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 600 \tLoss: tensor(1.8633, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 601 \tLoss: tensor(1.9324, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 602 \tLoss: tensor(1.9136, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 603 \tLoss: tensor(2.0153, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 604 \tLoss: tensor(2.0541, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 605 \tLoss: tensor(1.9795, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 606 \tLoss: tensor(1.9047, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 607 \tLoss: tensor(1.8419, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 608 \tLoss: tensor(1.8916, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 609 \tLoss: tensor(1.9924, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 610 \tLoss: tensor(1.8617, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 611 \tLoss: tensor(1.8613, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 612 \tLoss: tensor(1.9577, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 613 \tLoss: tensor(1.9262, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 614 \tLoss: tensor(1.9571, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 615 \tLoss: tensor(1.9027, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 616 \tLoss: tensor(1.8429, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 617 \tLoss: tensor(1.9310, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 618 \tLoss: tensor(1.9773, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 619 \tLoss: tensor(1.9033, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 620 \tLoss: tensor(1.9372, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 621 \tLoss: tensor(1.8516, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 622 \tLoss: tensor(1.8604, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 623 \tLoss: tensor(1.9838, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 624 \tLoss: tensor(1.9810, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "optimisers = [\n",
    "    {\n",
    "        'optimiser_class': torch.optim.SGD, \n",
    "        'tag': 'SGD'\n",
    "    },\n",
    "    {\n",
    "        'optimiser_class': torch.optim.Adam,\n",
    "        'tag': 'Adam'\n",
    "    },\n",
    "    {\n",
    "        'optimiser_class': torch.optim.Adagrad,\n",
    "        'tag': 'Adagrad'\n",
    "    },\n",
    "    {\n",
    "        'optimiser_class': torch.optim.RMSprop,\n",
    "        'tag': 'RMSProp'\n",
    "    }\n",
    "]\n",
    "\n",
    "learning_rates = [0.01, 0.001, 0.0001, 0.00001]\n",
    "\n",
    "for optimiser_obj in optimisers:   \n",
    "    for lr in learning_rates:\n",
    "        my_nn = NN([784, 1024, 1024, 512, 10], distribution=True, flatten_input=True)\n",
    "        optimiser_class = optimiser_obj['optimiser_class']\n",
    "        optimiser = optimiser_class(my_nn.parameters(), lr=lr)\n",
    "        tag = optimiser_obj['tag']\n",
    "        train(my_nn, optimiser, graph_name=lr, epochs=1, tag=f'Loss/Train/{tag}')\n",
    "    "
   ]
  },
  {
   "source": [
    "## Implementing our own PyTorch optimiser\n",
    "\n",
    "To understand a bit further what's happening under the hood, let's implement SGD from scratch."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD():\n",
    "    def __init__(self, model_params, learning_rate):\n",
    "        self.model_params = list(model_params) # HACK turning to list prevents len model_params being zero\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for param in self.model_params:\n",
    "                param -= self.learning_rate * param.grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.model_params:\n",
    "            if param.grad is None: # if not yet set (loss.backward() not yet called)\n",
    "                print('continuing')\n",
    "                continue\n",
    "            param.grad = torch.zeros_like(param.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " \tLoss: tensor(2.2885, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 340 \tLoss: tensor(2.2910, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 341 \tLoss: tensor(2.2927, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 342 \tLoss: tensor(2.2886, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 343 \tLoss: tensor(2.2922, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 344 \tLoss: tensor(2.2952, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 345 \tLoss: tensor(2.2884, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 346 \tLoss: tensor(2.2925, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 347 \tLoss: tensor(2.2874, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 348 \tLoss: tensor(2.2951, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 349 \tLoss: tensor(2.2917, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 350 \tLoss: tensor(2.2892, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 351 \tLoss: tensor(2.2932, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 352 \tLoss: tensor(2.2838, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 353 \tLoss: tensor(2.2909, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 354 \tLoss: tensor(2.2941, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 355 \tLoss: tensor(2.2861, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 356 \tLoss: tensor(2.2898, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 357 \tLoss: tensor(2.2870, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 358 \tLoss: tensor(2.2944, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 359 \tLoss: tensor(2.2941, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 360 \tLoss: tensor(2.2917, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 361 \tLoss: tensor(2.2895, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 362 \tLoss: tensor(2.2951, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 363 \tLoss: tensor(2.2884, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 364 \tLoss: tensor(2.2971, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 365 \tLoss: tensor(2.2935, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 366 \tLoss: tensor(2.2931, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 367 \tLoss: tensor(2.2840, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 368 \tLoss: tensor(2.2930, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 369 \tLoss: tensor(2.2937, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 370 \tLoss: tensor(2.2897, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 371 \tLoss: tensor(2.2959, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 372 \tLoss: tensor(2.2922, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 373 \tLoss: tensor(2.2950, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 374 \tLoss: tensor(2.2939, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 375 \tLoss: tensor(2.2887, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 376 \tLoss: tensor(2.2933, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 377 \tLoss: tensor(2.2920, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 378 \tLoss: tensor(2.2986, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 379 \tLoss: tensor(2.2802, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 380 \tLoss: tensor(2.2973, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 381 \tLoss: tensor(2.2904, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 382 \tLoss: tensor(2.2945, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 383 \tLoss: tensor(2.2824, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 384 \tLoss: tensor(2.2906, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 385 \tLoss: tensor(2.2886, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 386 \tLoss: tensor(2.2912, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 387 \tLoss: tensor(2.2938, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 388 \tLoss: tensor(2.2970, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 389 \tLoss: tensor(2.2906, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 390 \tLoss: tensor(2.2810, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 391 \tLoss: tensor(2.2913, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 392 \tLoss: tensor(2.2881, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 393 \tLoss: tensor(2.2729, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 394 \tLoss: tensor(2.2924, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 395 \tLoss: tensor(2.2868, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 396 \tLoss: tensor(2.2869, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 397 \tLoss: tensor(2.2838, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 398 \tLoss: tensor(2.2753, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 399 \tLoss: tensor(2.2869, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 400 \tLoss: tensor(2.2910, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 401 \tLoss: tensor(2.2953, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 402 \tLoss: tensor(2.2857, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 403 \tLoss: tensor(2.2872, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 404 \tLoss: tensor(2.2911, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 405 \tLoss: tensor(2.2881, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 406 \tLoss: tensor(2.2835, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 407 \tLoss: tensor(2.2834, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 408 \tLoss: tensor(2.2913, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 409 \tLoss: tensor(2.2856, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 410 \tLoss: tensor(2.2808, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 411 \tLoss: tensor(2.2753, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 412 \tLoss: tensor(2.2906, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 413 \tLoss: tensor(2.2938, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 414 \tLoss: tensor(2.2738, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 415 \tLoss: tensor(2.2849, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 416 \tLoss: tensor(2.2854, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 417 \tLoss: tensor(2.2921, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 418 \tLoss: tensor(2.2765, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 419 \tLoss: tensor(2.2966, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 420 \tLoss: tensor(2.2897, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 421 \tLoss: tensor(2.2851, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 422 \tLoss: tensor(2.2941, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 423 \tLoss: tensor(2.2880, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 424 \tLoss: tensor(2.2719, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 425 \tLoss: tensor(2.2812, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 426 \tLoss: tensor(2.2725, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 427 \tLoss: tensor(2.2743, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 428 \tLoss: tensor(2.2697, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 429 \tLoss: tensor(2.2869, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 430 \tLoss: tensor(2.2888, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 431 \tLoss: tensor(2.2831, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 432 \tLoss: tensor(2.2729, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 433 \tLoss: tensor(2.2987, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 434 \tLoss: tensor(2.2765, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 435 \tLoss: tensor(2.2741, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 436 \tLoss: tensor(2.2879, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 437 \tLoss: tensor(2.2726, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 438 \tLoss: tensor(2.2955, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 439 \tLoss: tensor(2.3003, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 440 \tLoss: tensor(2.2729, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 441 \tLoss: tensor(2.2789, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 442 \tLoss: tensor(2.2812, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 443 \tLoss: tensor(2.2948, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 444 \tLoss: tensor(2.2450, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 445 \tLoss: tensor(2.2899, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 446 \tLoss: tensor(2.2503, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 447 \tLoss: tensor(2.2390, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 448 \tLoss: tensor(2.2730, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 449 \tLoss: tensor(2.2979, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 450 \tLoss: tensor(2.2588, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 451 \tLoss: tensor(2.2488, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 452 \tLoss: tensor(2.2946, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 453 \tLoss: tensor(2.2713, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 454 \tLoss: tensor(2.2626, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 455 \tLoss: tensor(2.2596, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 456 \tLoss: tensor(2.2579, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 457 \tLoss: tensor(2.2514, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 458 \tLoss: tensor(2.2138, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 459 \tLoss: tensor(2.2474, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 460 \tLoss: tensor(2.2983, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 461 \tLoss: tensor(2.1922, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 462 \tLoss: tensor(2.2031, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 463 \tLoss: tensor(2.2408, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 464 \tLoss: tensor(2.2251, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 465 \tLoss: tensor(2.2541, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 466 \tLoss: tensor(2.2149, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 467 \tLoss: tensor(2.2334, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 468 \tLoss: tensor(2.2501, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 469 \tLoss: tensor(2.3009, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 470 \tLoss: tensor(2.2625, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 471 \tLoss: tensor(2.3167, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 472 \tLoss: tensor(2.2627, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 473 \tLoss: tensor(2.2237, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 474 \tLoss: tensor(2.2139, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 475 \tLoss: tensor(2.2729, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 476 \tLoss: tensor(2.3003, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 477 \tLoss: tensor(2.2592, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 478 \tLoss: tensor(2.1750, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 479 \tLoss: tensor(2.1915, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 480 \tLoss: tensor(2.1726, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 481 \tLoss: tensor(2.1813, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 482 \tLoss: tensor(2.2682, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 483 \tLoss: tensor(2.2631, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 484 \tLoss: tensor(2.2735, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 485 \tLoss: tensor(2.2840, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 486 \tLoss: tensor(2.2286, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 487 \tLoss: tensor(2.2727, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 488 \tLoss: tensor(2.2216, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 489 \tLoss: tensor(2.1976, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 490 \tLoss: tensor(2.2602, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 491 \tLoss: tensor(2.2920, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 492 \tLoss: tensor(2.2662, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 493 \tLoss: tensor(2.2759, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 494 \tLoss: tensor(2.3030, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 495 \tLoss: tensor(2.2383, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 496 \tLoss: tensor(2.2089, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 497 \tLoss: tensor(2.2406, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 498 \tLoss: tensor(2.1731, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 499 \tLoss: tensor(2.2844, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 500 \tLoss: tensor(2.2222, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 501 \tLoss: tensor(2.1532, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 502 \tLoss: tensor(2.2176, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 503 \tLoss: tensor(2.2637, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 504 \tLoss: tensor(2.3007, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 505 \tLoss: tensor(2.2572, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 506 \tLoss: tensor(2.1749, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 507 \tLoss: tensor(2.2481, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 508 \tLoss: tensor(2.2570, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 509 \tLoss: tensor(2.2944, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 510 \tLoss: tensor(2.2889, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 511 \tLoss: tensor(2.1150, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 512 \tLoss: tensor(2.1312, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 513 \tLoss: tensor(2.1172, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 514 \tLoss: tensor(2.2032, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 515 \tLoss: tensor(2.1053, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 516 \tLoss: tensor(2.2208, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 517 \tLoss: tensor(2.2690, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 518 \tLoss: tensor(2.1895, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 519 \tLoss: tensor(2.2035, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 520 \tLoss: tensor(2.3050, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 521 \tLoss: tensor(2.2989, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 522 \tLoss: tensor(2.2032, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 523 \tLoss: tensor(2.2853, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 524 \tLoss: tensor(2.2279, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 525 \tLoss: tensor(2.2957, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 526 \tLoss: tensor(2.1784, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 527 \tLoss: tensor(2.2916, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 528 \tLoss: tensor(2.2034, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 529 \tLoss: tensor(2.2031, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 530 \tLoss: tensor(2.2476, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 531 \tLoss: tensor(2.1746, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 532 \tLoss: tensor(2.2883, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 533 \tLoss: tensor(2.1902, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 534 \tLoss: tensor(2.1908, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 535 \tLoss: tensor(2.2020, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 536 \tLoss: tensor(2.3069, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 537 \tLoss: tensor(2.2486, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 538 \tLoss: tensor(2.2250, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 539 \tLoss: tensor(2.2074, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 540 \tLoss: tensor(2.2488, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 541 \tLoss: tensor(2.2466, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 542 \tLoss: tensor(2.2797, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 543 \tLoss: tensor(2.2963, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 544 \tLoss: tensor(2.2227, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 545 \tLoss: tensor(2.2810, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 546 \tLoss: tensor(2.2864, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 547 \tLoss: tensor(2.1470, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 548 \tLoss: tensor(2.2825, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 549 \tLoss: tensor(2.2497, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 550 \tLoss: tensor(2.2961, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 551 \tLoss: tensor(2.0680, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 552 \tLoss: tensor(2.2905, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 553 \tLoss: tensor(2.2046, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 554 \tLoss: tensor(2.2771, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 555 \tLoss: tensor(2.2758, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 556 \tLoss: tensor(2.2166, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 557 \tLoss: tensor(2.1335, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 558 \tLoss: tensor(2.2408, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 559 \tLoss: tensor(2.2367, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 560 \tLoss: tensor(2.1788, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 561 \tLoss: tensor(2.2403, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 562 \tLoss: tensor(2.1681, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 563 \tLoss: tensor(2.2776, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 564 \tLoss: tensor(2.1903, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 565 \tLoss: tensor(2.1743, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 566 \tLoss: tensor(2.2217, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 567 \tLoss: tensor(2.1331, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 568 \tLoss: tensor(2.1670, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 569 \tLoss: tensor(2.2463, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 570 \tLoss: tensor(2.1797, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 571 \tLoss: tensor(2.2816, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 572 \tLoss: tensor(2.1703, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 573 \tLoss: tensor(2.1227, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 574 \tLoss: tensor(2.2406, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 575 \tLoss: tensor(2.2459, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 576 \tLoss: tensor(2.2296, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 577 \tLoss: tensor(2.1185, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 578 \tLoss: tensor(2.1515, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 579 \tLoss: tensor(2.2138, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 580 \tLoss: tensor(2.2331, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 581 \tLoss: tensor(2.2001, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 582 \tLoss: tensor(2.1394, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 583 \tLoss: tensor(2.2718, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 584 \tLoss: tensor(2.2107, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 585 \tLoss: tensor(2.1506, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 586 \tLoss: tensor(2.2099, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 587 \tLoss: tensor(2.2721, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 588 \tLoss: tensor(2.2429, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 589 \tLoss: tensor(2.2459, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 590 \tLoss: tensor(2.2450, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 591 \tLoss: tensor(2.1006, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 592 \tLoss: tensor(2.1371, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 593 \tLoss: tensor(2.1488, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 594 \tLoss: tensor(2.1928, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 595 \tLoss: tensor(2.2904, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 596 \tLoss: tensor(2.0469, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 597 \tLoss: tensor(2.1027, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 598 \tLoss: tensor(2.1751, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 599 \tLoss: tensor(2.2005, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 600 \tLoss: tensor(2.1882, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 601 \tLoss: tensor(2.0894, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 602 \tLoss: tensor(2.2646, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 603 \tLoss: tensor(2.1732, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 604 \tLoss: tensor(2.2456, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 605 \tLoss: tensor(2.1877, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 606 \tLoss: tensor(2.1821, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 607 \tLoss: tensor(2.1454, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 608 \tLoss: tensor(2.2080, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 609 \tLoss: tensor(2.1271, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 610 \tLoss: tensor(2.1932, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 611 \tLoss: tensor(2.1789, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 612 \tLoss: tensor(2.0784, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 613 \tLoss: tensor(2.1689, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 614 \tLoss: tensor(2.1972, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 615 \tLoss: tensor(2.1853, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 616 \tLoss: tensor(2.0932, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 617 \tLoss: tensor(2.1994, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 618 \tLoss: tensor(2.0982, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 619 \tLoss: tensor(2.2743, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 620 \tLoss: tensor(2.0298, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 621 \tLoss: tensor(2.0623, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 622 \tLoss: tensor(2.1980, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 623 \tLoss: tensor(2.1725, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 624 \tLoss: tensor(2.1662, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "my_nn = NN([784, 1024, 1024, 512, 10], distribution=True, flatten_input=True)\n",
    "optimiser = SGD(my_nn.parameters(), learning_rate=0.1)\n",
    "\n",
    "train(my_nn, optimiser, 'Loss/Train/custom_sgd')"
   ]
  },
  {
   "source": [
    "## Challenges\n",
    "- flash card match images with name of optimisation algorithm\n",
    "- roughly sketch the paths that different optimisation algorithms might take"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}