{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "## Prerequisites\n",
    "- [Classification](https://github.com/AI-Core/Strong-ML-Foundations/blob/master/Classification.ipynb)\n",
    "\n",
    "## Why do simple models struggle with meaningful tasks?\n",
    "\n",
    "Whereas the size of a house and its price might be linearly correlated, the pixel intensities of an image are certainly not linearly correlated with whether it contains a dog or a cat.\n",
    "\n",
    "![](./images/complex-fn.png)\n",
    "\n",
    "We need to build much more complex models to solve harder problems.\n",
    "\n",
    "## Can we build more complex models by combining many simple transformations?\n",
    "\n",
    "The models we have just seen apply a single transformation to the data. However most problems of practical interest can't be solved using such simple models. \n",
    "\n",
    "Models with greater **capacity** are those which are able to model more complicated functions.\n",
    "\n",
    "A single linear transformation (multiplication by weights of model) stretches the input space by a certain factor in some direction, and adding a constant (bias) shifts it. \n",
    "We call models which apply  \n",
    "What if we applied more than one **layer** of transformations to our inputs, to create a **deep model**. Would we be able to increase the capacity of our model and make it able to model more complex input-output relationships, particularly non-linear ones?\n",
    "\n",
    "![](./images/shallow-vs-deep.png)\n",
    "\n",
    "...well, not quite yet.\n",
    "\n",
    "...if we repeatedly apply a linear transformation, the input can be factorised out of the output, showing that many repeated linear functions are eventually equal to a single linear transformation.\n",
    "\n",
    "![](./images/factor-proof.png)\n",
    "\n",
    "## So how can we increase the capacity of our models?\n",
    "\n",
    "We want to be able to model non-linear functions, so let's try to throw in some non-linear transformations into our model.\n",
    "\n",
    "![](./images/activation.png)\n",
    "\n",
    "These non-linear functions prevent the input being able to be factorised out of the model. Hence the overall transformation can represent non-linear input-output relationships.\n",
    "\n",
    "We call these non-linear functions **activation functions**.\n",
    "\n",
    "However, It's not like we want to introduce really complicated functions into our model - ideally we wouldn't even have to and we could keep things simple. So let's try and complicate things only a minimal amount by keeping our activation functions very simple.\n",
    "\n",
    "Here are some common activation functions. ReLU (Rectified Linear Unit) is by far the most widely used.\n",
    "\n",
    "![](./images/activ-fns.png)\n",
    "\n",
    "Now we have all of the ingredients to fully understand how we can model more complicated functions. Let's look at that all together:\n",
    "\n",
    "![](./images/full-nn.png)\n",
    "\n",
    "Guess what? That is a **neural network**. Surprise.\n",
    "\n",
    "It's just repeated simple linear transformations followed by simple non-linear transformations (activation functions). Simple.\n",
    "\n",
    "Let's learn some jargon.\n",
    "\n",
    "![](./images/nn.png)\n",
    "\n",
    "Neural networks have additional hyperparameters of the depth of the model and the width of each layer.\n",
    "\n",
    "## Let's play around with a trained neural network to see what nodes activate (have large values) when we change the input.\n",
    "\n",
    "Follow [this link](http://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.97988&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false) to play around with a neural network.\n",
    "\n",
    "## What can neural networks do?\n",
    "\n",
    "The motivation that led us to deriving neural networks was that we wanted to model more complex functions. But what functions can a neural network actually represent? Well, as we show below they can actually represent almost all continuous functions. Neural Networks are **general function approximators**.\n",
    "\n",
    "![](./images/univ-approx.png)\n",
    "\n",
    "## How can our neural networks learn to model some function?\n",
    "\n",
    "As we did in the optimisation notebook, we can adjust our model parameters using gradient descent as such:\n",
    "1. Pass input data forward through model to output a prediction\n",
    "2. Calculate loss between predicted output and output label\n",
    "3. Find direction that moving the model parameters in will reduce the error\n",
    "4. Move model weights (parameters) a small amount in that direction \n",
    "\n",
    "![](./images/backprop.png)\n",
    "\n",
    "Here you can see that many terms reappear when computing the gradients of preceeding layers. \n",
    "By caching those terms (saving them in memory for use later), we save having to recompute them for these layers nearer the input. This makes finding the gradients of the loss with respect to each weight in the model much more efficient both in terms of memory and speed. \n",
    "This process of computing these gradients effectively is called the **backpropagation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's prepare our data\n",
    "\n",
    "As we did previously, let's get the dataloaders for the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import get_dataloaders\n",
    "\n",
    "train_loader, val_loader, test_loader = get_dataloaders()"
   ]
  },
  {
   "source": [
    "Let's visualise our data to remind ourselves of how it looks"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Images:\nshape: torch.Size([16, 1, 28, 28])\nLabels:\nshape: torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "batch = list(train_loader)[0] # get first example\n",
    "images, labels = batch # unpack example\n",
    "\n",
    "print('Images:')\n",
    "print('shape:', images.shape)\n",
    "\n",
    "print('Labels:')\n",
    "print('shape:', labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a neural network with PyTorch\n",
    "\n",
    "As we saw previously, PyTorch makes it really easy for us to build complex models that can be improved via gradient based optimisation. It does this by providing a class named `torch.nn.Module`. Our model classes should inherit from this class because it does a few very useful things for us, including:\n",
    "\n",
    "1. `torch.nn.Module` keeps track of all `torch.nn.Parameters` that are created within it. So when we add a linear layer to our model, the parameters (matrix of weights) in that layer will be added to a list of our model's parameters. We can retrieve all parameters of our model using its `parameters()` method. We will later pass this (`mymodel.parameters()`) to our optimiser when we tell it that *this* is what it should be optimising.\n",
    "\n",
    "\n",
    "2. `torch.nn.Module` treats the `forward` method (function) of any child class specially by assigning it to the `__call__` method. That means that running `mymodel.forward(some_data)` is equal to `mymodel(some_data)`. \n",
    "\n",
    "\n",
    "It contains many more useful tools\n",
    "\n",
    "[More detail](https://pytorch.org/tutorials/beginner/nn_tutorial.html) on `torch.nn.Module`\n",
    "Check out the docs [here]()\n",
    "\n",
    "Once we have created a class to represent our model, we need to define how it performs the forward pass. What layers of transformations do we need to give it? \n",
    "Check out these [docs](https://pytorch.org/docs/stable/nn.html#linear-layers) to look at all the layers PyTorch provides.\n",
    "Hint: what layer have I linked to?\n",
    "\n",
    "After we've defined some layers for our model we should implement the forward function that will define what happens when we call an instance of our class. This should pass the argument (our input data) through each of the layers, and apply an activation function to them between each, before returning the transformed input as the output. The output should represent a categorical probability distribution over which class the input belongs to. What shape does it need to be? What function does it need to have applied to it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F \n",
    "class NN(torch.nn.Module): # create a neural network class\n",
    "    def __init__(self): # initialiser\n",
    "        super().__init__() # initialise the parent class\n",
    "        self.layer1 = torch.nn.Linear(784, 1024) # create our first linear layer\n",
    "        self.layer2 = torch.nn.Linear(1024, 256) # create our second linear layer\n",
    "        self.layer3 = torch.nn.Linear(256, 10) # create our third linear layer\n",
    "        \n",
    "    def forward(self, x): # define the forward pass\n",
    "        x = x.view(-1, 784) # flatten out our image features into vectors\n",
    "        x = self.layer1(x) # pass through the first linear layer\n",
    "        x = F.relu(x) # apply activation function\n",
    "        x = self.layer2(x) # pass through the second linear layer\n",
    "        x = F.relu(x) # apply activation function\n",
    "        x = self.layer3(x) # pass through the third linear layer\n",
    "        x = F.softmax(x) # apply activation function\n",
    "        return x # return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the neural network and visualising it's performance\n",
    "\n",
    "Now we've actually made a template for our model, we can actually\n",
    "- instantiate it by creating an instance of it from our class template\n",
    "- define how we will improve it by specifying an optimiser\n",
    "- define how we will measure its performance by specifying a criterion\n",
    "- train it\n",
    "- write its loss to a graph and see how this changes as it continues to train\n",
    "\n",
    "Let's code that up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET UP TENSORBOARD TO VIEW TRAINING CURVES WITHIN THE NOTEBOOK\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "my_nn = NN()\n",
    "\n",
    "# CREATE OUR OPTIMISER\n",
    "optimiser = torch.optim.Adam(              # what optimiser should we use?\n",
    "    myNeuralNetwork.parameters(),          # what should it optimise?\n",
    "    lr=learning_rate                       # using what learning rate?\n",
    ")\n",
    "\n",
    "# CREATE OUR CRITERION\n",
    "criterion = torch.nn.CrossEntropyLoss()             # callable class that compares our predictions to our labels and returns our loss\n",
    "\n",
    "# SET UP TRAINING VISUALISATION\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()                            # we will use this to show our models performance on a graph\n",
    "    \n",
    "# TRAINING LOOP\n",
    "def train(model, epochs):\n",
    "    model.train()                                  # put the model into training mode (more on this later)\n",
    "    for epoch in range(epochs):\n",
    "        for idx, minibatch in enumerate(train_loader):\n",
    "            inputs, labels = minibatch\n",
    "            prediction = model(inputs)             # pass the data forward through the model\n",
    "            loss = criterion(prediction, labels)   # compute the loss\n",
    "            print('Epoch:', epoch, '\\tBatch:', idx, '\\tLoss:', loss)\n",
    "            optimiser.zero_grad()                  # reset the gradients attribute of each of the model's params to zero\n",
    "            loss.backward()                        # backward pass to compute and set all of the model param's gradients\n",
    "            optimiser.step()                       # update the model's parameters\n",
    "            writer.add_scalar('Loss/Train', loss, epoch*len(train_loader) + idx)    # write loss to a graph\n",
    "\n",
    "train(my_nn, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does the loss actually mean practically?\n",
    "\n",
    "The absolute value of the loss doesn't really mean much, it's just a way of continuously evaluating the relative performance of the model whilst it trains. The real metric of performance that we care about is the proportion of ***unseen*** examples that our neural network can correctly classify. These unseen examples are what the test loader consists of.\n",
    "\n",
    "Let's write the code to calculate that now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "            \n",
    "def test(model):\n",
    "    num_correct = 0\n",
    "    num_examples = len(test_data)                       # test DATA not test LOADER\n",
    "    for inputs, labels in test_loader:                  # for all exampls, over all mini-batches in the test dataset\n",
    "        predictions = model(inputs)\n",
    "        predictions = torch.max(predictions, axis=1)    # reduce to find max indices along direction which column varies\n",
    "        predictions = predictions[1]                    # torch.max returns (values, indices)\n",
    "        num_correct += int(sum(predictions == labels))\n",
    "    percent_correct = num_correct / num_examples * 100\n",
    "    print('Accuracy:', percent_correct)\n",
    "    \n",
    "test(myNeuralNetwork)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "1. Compare the loss curves generated by using different batch sizes. What's the best? As you change the batch size, what variable do you need to change to give those curves the same domain over the x-axis (num writes to summary writer)\n",
    "2. It would be good to validate our model as we go along to ensure that we don't overfit. Let's write a training loop that tests the loss on the validation set after each epoch. Plot the validation error alongside What can you see on the graphs that indicates overfitting?\n",
    "3. What is the best accuracy you can achieve? Can you implement a grid search and a random search to try them automatically. Record all permutations that you try.\n",
    "4. What feature of the input data is our standard neural network not taking advantage of? Hint: '************* neural networks' take this into account.\n",
    "\n",
    "## Congratulations you boss, you've finished the notebook!\n",
    "\n",
    "Please provide your feedback [here](https://docs.google.com/forms/d/e/1FAIpQLSdZSxvkAE19vjDN4jpp0VvUBPGr_wdtayGAcRNfFGH7e7jQDQ/viewform?usp=sf_link). It means a lot to us.\n",
    "\n",
    "Next, you might want to check out:\n",
    "- [Convolutional Neural Networks](https://github.com/AI-Core/Convolutional-Neural-Networks/blob/master/Convolutional%20Neural%20Networks.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('main': conda)",
   "display_name": "Python 3.8.5 64-bit ('main': conda)",
   "metadata": {
    "interpreter": {
     "hash": "06c1e258a470a687113bfba03f207c092b27379067ada2d83b8b31269ab641fe"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}