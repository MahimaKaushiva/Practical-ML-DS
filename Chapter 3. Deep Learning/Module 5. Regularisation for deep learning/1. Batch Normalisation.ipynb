{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "- Formula behind batch normalisation\n",
    "- Application in PyTorch\n",
    "- Advantage of batch normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "- Data normalisation\n",
    "- Feed-forward neural networks\n",
    "- Convolutional neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro\n",
    "\n",
    "Firstly, [here's](https://arxiv.org/abs/1502.03167) a link to the original dropout paper.\n",
    "\n",
    "Before training a machine learning algorithm, it is common practice to normalise the input data, especially when the features have different scales, e.g. house prices and building year. This can lead to features with higher values having an unwanted greater impact on changes of a predictor. Normalising data can avoid this and lead to better performance. Since this technique is proven to work for the input data, it is natural to apply the same technique to the hidden layers in a neural network. <br>\n",
    "Batch normalisation tackles the problem of internal covariate shift in deep neural networks, which describes the phenomenon that the input distribution of each layer changes a lot as the input to each layer is affected by the parameters of all preceding layers s.t. even small changes in parameters have a big impact on the parameters further down the line. Solving this problem typically requires careful initialisation of the parameters as well as small learning rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Notation\n",
    "Batch normalisation basically sets the mean of each feature to zero and the variance to 1:\n",
    "\\begin{equation*}\n",
    "\\hat{x} = \\frac{x - \\mu(x)}{\\sqrt{\\sigma^2(x)}}\n",
    "\\end{equation*}\n",
    "where the mean and variance are computed over the batch during training and over the population after training.<br>\n",
    "The normalised features are then scaled and shifted by introducing the parameters $\\gamma$ and $\\beta$:\n",
    "\\begin{equation*}\n",
    "y = \\gamma \\hat{x} + \\beta\n",
    "\\end{equation*}\n",
    "These steps are applied to the activations of each layer before feeding them as input to the next layer. Therefore, we must also include them in the backpropagation, which we do by calculating the gradient w.r.t. the new parameters $\\gamma$ and $\\beta$. \n",
    "These parameters are important because the computation of $\\hat{x}$ where the mean is set to 0 with unit variance might not be desirable in every layer. Especially if we compute this for the input of a softmax function, it is desirable that the input has higher variance such that the output is a conclusive probability distribution over its input. The $\\gamma$ and $\\beta$ parameters learn to regulate $\\hat{x}$. If $\\gamma$ is set to $\\sqrt{\\sigma^2(x)}$ and $\\beta$ is set to $\\mu(x)$, we can restore the original values of $x$.\n",
    "<br><br>\n",
    "\n",
    "In a convolutional neural network, the normalisation is applied jointly over all locations in a feature map, s.t. we can learn the parameters $\\gamma$ and $\\beta$ per feature map and not per activation because we want to normalise the features in the same way regardless of whether they are in different convolutional windows. This means that we normalise the same activations in the same feature maps.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "We will use the CNN for MNIST image prediction that you already worked with before. Run the original code so you can compare performance to the same CNN using batch normalisation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import get_dataloaders\n",
    "import torch\n",
    "\n",
    "train_loader, val_loader, test_loader = get_dataloaders()"
   ]
  },
  {
   "source": [
    "Below, we create two CNN classes - one which has batch normalisation layers, and one which doesnt. We'll compare the performance of each of them."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class BatchNormCNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_layers = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 16, kernel_size=5, stride=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm2d(16), # HERE IS A BATCH NORM LAYER\n",
    "            torch.nn.Conv2d(16, 32, kernel_size=5, stride=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm2d(32) # HERE IS A BATCH NORM LAYER\n",
    "        )\n",
    "        self.fc_layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(32*20*20, 10) # put your linear architecture here using torch.nn.Sequential \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)# pass through conv layers\n",
    "        x = x.view(x.shape[0], -1)# flatten output ready for fully connected layer\n",
    "        x = self.fc_layers(x)# pass through fully connected layer\n",
    "        x = F.softmax(x, dim=1)# softmax activation function on outputs\n",
    "        return x\n",
    "\n",
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_layers = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 16, kernel_size=5, stride=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(16, 32, kernel_size=5, stride=1),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.fc_layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(32*20*20, 10)\n",
    "        )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)# pass through conv layers\n",
    "        x = x.view(x.shape[0], -1)# flatten output ready for fully connected layer\n",
    "        x = self.fc_layers(x)# pass through fully connected layer\n",
    "        x = F.softmax(x, dim=1)# softmax activation function on outputs\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0005 # set learning rate\n",
    "\n",
    "cnn_without_bn = CNN() #instantiate model\n",
    "cnn_with_bn = BatchNormCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 0 \tLoss: 1.4817818403244019\n",
      "Epoch: 1 \tLoss: 1.478590965270996\n",
      "Training Complete. Final loss = 1.4611504077911377\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'cnn_with_bn' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ef436373980d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_without_bn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'without batch norm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_with_bn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'with_batch_norm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'cnn_with_bn' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def train(model, lr, tag, epochs=2, verbose=True):\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate) # create optimiser\n",
    "    criterion = torch.nn.CrossEntropyLoss() #use cross entropy loss function\n",
    "    writer = SummaryWriter(log_dir=\"../../runs/Batch Norm Experiments\") \n",
    "    batch_idx = 0\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for idx, (inputs, labels) in enumerate(train_loader):\n",
    "            prediction = model(inputs) # pass the data forward through the model\n",
    "            loss = criterion(prediction, labels) # compute the cost\n",
    "            epoch_loss += loss\n",
    "            optimiser.zero_grad() # reset the gradients attribute of all of the model's params to zero\n",
    "            loss.backward() # backward pass to compute and store all of the model's param's gradients\n",
    "            optimiser.step() # update the model's parameters\n",
    "            writer.add_scalar(f'Batch Norm Experiments/Loss-{tag}', loss, batch_idx)    # write loss to a graph\n",
    "            batch_idx += 1\n",
    "        epoch_loss /= len(train_loader)\n",
    "        print('Epoch:', epoch, '\\tLoss:', epoch_loss.item())\n",
    "        \n",
    "    print('Training Complete. Final loss =',loss.item())\n",
    "    \n",
    "train(cnn_without_bn, learning_rate, 'without batch norm')\n",
    "train(cnn_with_bn, learning_rate, 'with_batch_norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train Accuracy: 97.57000000000001\n",
      "Validation Accuracy: 96.96000000000001\n",
      "Test Accuracy: 97.41\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "            \n",
    "def calc_accuracy(model, dataloader):\n",
    "    model.eval()\n",
    "    num_correct = 0\n",
    "    num_examples = len(dataloader.dataset)                       # test DATA not test LOADER\n",
    "    for inputs, labels in dataloader:                  # for all exampls, over all mini-batches in the test dataset\n",
    "        predictions = model(inputs)\n",
    "        predictions = torch.max(predictions, axis=1)    # reduce to find max indices along direction which column varies\n",
    "        predictions = predictions[1]                    # torch.max returns (values, indices)\n",
    "        num_correct += int(sum(predictions == labels))\n",
    "    percent_correct = num_correct / num_examples * 100\n",
    "    return percent_correct\n",
    "\n",
    "print('WITHOUT BN')\n",
    "print('Train Accuracy:', calc_accuracy(cnn_without_bn, train_loader))\n",
    "print('Validation Accuracy:', calc_accuracy(cnn_without_bn, val_loader))\n",
    "print('Test Accuracy:', calc_accuracy(cnn_without_bn, test_loader))\n",
    "\n",
    "print('WITH BN')\n",
    "print('Train Accuracy:', calc_accuracy(cnn_with_bn, train_loader))\n",
    "print('Validation Accuracy:', calc_accuracy(cnn_with_bn, val_loader))\n",
    "print('Test Accuracy:', calc_accuracy(cnn_with_bn, test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Compare to cnn without batch normalisation <br>\n",
    "Use a larger lr with batch normalisation\n",
    "Add plots to show stability and fast convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "4 \tLoss: 1.4690825939178467\nEpoch: 3 \tBatch: 355 \tLoss: 1.504518985748291\nEpoch: 3 \tBatch: 356 \tLoss: 1.473148226737976\nEpoch: 3 \tBatch: 357 \tLoss: 1.4720790386199951\nEpoch: 3 \tBatch: 358 \tLoss: 1.4752596616744995\nEpoch: 3 \tBatch: 359 \tLoss: 1.462831735610962\nEpoch: 3 \tBatch: 360 \tLoss: 1.4618823528289795\nEpoch: 3 \tBatch: 361 \tLoss: 1.5017436742782593\nEpoch: 3 \tBatch: 362 \tLoss: 1.4638078212738037\nEpoch: 3 \tBatch: 363 \tLoss: 1.4766042232513428\nEpoch: 3 \tBatch: 364 \tLoss: 1.462192177772522\nEpoch: 3 \tBatch: 365 \tLoss: 1.4612401723861694\nEpoch: 3 \tBatch: 366 \tLoss: 1.4644782543182373\nEpoch: 3 \tBatch: 367 \tLoss: 1.4747031927108765\nEpoch: 3 \tBatch: 368 \tLoss: 1.4773313999176025\nEpoch: 3 \tBatch: 369 \tLoss: 1.4613133668899536\nEpoch: 3 \tBatch: 370 \tLoss: 1.46896231174469\nEpoch: 3 \tBatch: 371 \tLoss: 1.4839258193969727\nEpoch: 3 \tBatch: 372 \tLoss: 1.4717891216278076\nEpoch: 3 \tBatch: 373 \tLoss: 1.4728102684020996\nEpoch: 3 \tBatch: 374 \tLoss: 1.4613990783691406\nEpoch: 3 \tBatch: 375 \tLoss: 1.4664561748504639\nEpoch: 3 \tBatch: 376 \tLoss: 1.4691585302352905\nEpoch: 3 \tBatch: 377 \tLoss: 1.4716354608535767\nEpoch: 3 \tBatch: 378 \tLoss: 1.4768370389938354\nEpoch: 3 \tBatch: 379 \tLoss: 1.4704915285110474\nEpoch: 3 \tBatch: 380 \tLoss: 1.5089614391326904\nEpoch: 3 \tBatch: 381 \tLoss: 1.465684413909912\nEpoch: 3 \tBatch: 382 \tLoss: 1.4723973274230957\nEpoch: 3 \tBatch: 383 \tLoss: 1.4773693084716797\nEpoch: 3 \tBatch: 384 \tLoss: 1.4753626585006714\nEpoch: 3 \tBatch: 385 \tLoss: 1.467221736907959\nEpoch: 3 \tBatch: 386 \tLoss: 1.473514199256897\nEpoch: 3 \tBatch: 387 \tLoss: 1.4691200256347656\nEpoch: 3 \tBatch: 388 \tLoss: 1.4808262586593628\nEpoch: 3 \tBatch: 389 \tLoss: 1.4661245346069336\nEpoch: 3 \tBatch: 390 \tLoss: 1.4736868143081665\nEpoch: 4 \tBatch: 0 \tLoss: 1.4699130058288574\nEpoch: 4 \tBatch: 1 \tLoss: 1.4838626384735107\nEpoch: 4 \tBatch: 2 \tLoss: 1.471688985824585\nEpoch: 4 \tBatch: 3 \tLoss: 1.4676172733306885\nEpoch: 4 \tBatch: 4 \tLoss: 1.4636610746383667\nEpoch: 4 \tBatch: 5 \tLoss: 1.478004813194275\nEpoch: 4 \tBatch: 6 \tLoss: 1.4711556434631348\nEpoch: 4 \tBatch: 7 \tLoss: 1.4645925760269165\nEpoch: 4 \tBatch: 8 \tLoss: 1.4789557456970215\nEpoch: 4 \tBatch: 9 \tLoss: 1.4751670360565186\nEpoch: 4 \tBatch: 10 \tLoss: 1.4643656015396118\nEpoch: 4 \tBatch: 11 \tLoss: 1.480182409286499\nEpoch: 4 \tBatch: 12 \tLoss: 1.4676566123962402\nEpoch: 4 \tBatch: 13 \tLoss: 1.4840803146362305\nEpoch: 4 \tBatch: 14 \tLoss: 1.461391806602478\nEpoch: 4 \tBatch: 15 \tLoss: 1.4685808420181274\nEpoch: 4 \tBatch: 16 \tLoss: 1.4934583902359009\nEpoch: 4 \tBatch: 17 \tLoss: 1.4685758352279663\nEpoch: 4 \tBatch: 18 \tLoss: 1.4744129180908203\nEpoch: 4 \tBatch: 19 \tLoss: 1.4769973754882812\nEpoch: 4 \tBatch: 20 \tLoss: 1.482743740081787\nEpoch: 4 \tBatch: 21 \tLoss: 1.469278335571289\nEpoch: 4 \tBatch: 22 \tLoss: 1.4761624336242676\nEpoch: 4 \tBatch: 23 \tLoss: 1.4675010442733765\nEpoch: 4 \tBatch: 24 \tLoss: 1.4674922227859497\nEpoch: 4 \tBatch: 25 \tLoss: 1.4619505405426025\nEpoch: 4 \tBatch: 26 \tLoss: 1.46193528175354\nEpoch: 4 \tBatch: 27 \tLoss: 1.4777666330337524\nEpoch: 4 \tBatch: 28 \tLoss: 1.4804751873016357\nEpoch: 4 \tBatch: 29 \tLoss: 1.4653027057647705\nEpoch: 4 \tBatch: 30 \tLoss: 1.4633760452270508\nEpoch: 4 \tBatch: 31 \tLoss: 1.4680819511413574\nEpoch: 4 \tBatch: 32 \tLoss: 1.4689069986343384\nEpoch: 4 \tBatch: 33 \tLoss: 1.4657325744628906\nEpoch: 4 \tBatch: 34 \tLoss: 1.4644356966018677\nEpoch: 4 \tBatch: 35 \tLoss: 1.466165542602539\nEpoch: 4 \tBatch: 36 \tLoss: 1.4906924962997437\nEpoch: 4 \tBatch: 37 \tLoss: 1.4939953088760376\nEpoch: 4 \tBatch: 38 \tLoss: 1.4694846868515015\nEpoch: 4 \tBatch: 39 \tLoss: 1.4646704196929932\nEpoch: 4 \tBatch: 40 \tLoss: 1.475852608680725\nEpoch: 4 \tBatch: 41 \tLoss: 1.4689927101135254\nEpoch: 4 \tBatch: 42 \tLoss: 1.4795068502426147\nEpoch: 4 \tBatch: 43 \tLoss: 1.4695954322814941\nEpoch: 4 \tBatch: 44 \tLoss: 1.4615166187286377\nEpoch: 4 \tBatch: 45 \tLoss: 1.495262622833252\nEpoch: 4 \tBatch: 46 \tLoss: 1.4785284996032715\nEpoch: 4 \tBatch: 47 \tLoss: 1.4700078964233398\nEpoch: 4 \tBatch: 48 \tLoss: 1.4881207942962646\nEpoch: 4 \tBatch: 49 \tLoss: 1.4686962366104126\nEpoch: 4 \tBatch: 50 \tLoss: 1.4697456359863281\nEpoch: 4 \tBatch: 51 \tLoss: 1.461236596107483\nEpoch: 4 \tBatch: 52 \tLoss: 1.4612071514129639\nEpoch: 4 \tBatch: 53 \tLoss: 1.4611644744873047\nEpoch: 4 \tBatch: 54 \tLoss: 1.479928731918335\nEpoch: 4 \tBatch: 55 \tLoss: 1.4793695211410522\nEpoch: 4 \tBatch: 56 \tLoss: 1.4673962593078613\nEpoch: 4 \tBatch: 57 \tLoss: 1.4690046310424805\nEpoch: 4 \tBatch: 58 \tLoss: 1.4699926376342773\nEpoch: 4 \tBatch: 59 \tLoss: 1.4767377376556396\nEpoch: 4 \tBatch: 60 \tLoss: 1.4774693250656128\nEpoch: 4 \tBatch: 61 \tLoss: 1.477221965789795\nEpoch: 4 \tBatch: 62 \tLoss: 1.4812953472137451\nEpoch: 4 \tBatch: 63 \tLoss: 1.4627491235733032\nEpoch: 4 \tBatch: 64 \tLoss: 1.4613680839538574\nEpoch: 4 \tBatch: 65 \tLoss: 1.4686574935913086\nEpoch: 4 \tBatch: 66 \tLoss: 1.4768086671829224\nEpoch: 4 \tBatch: 67 \tLoss: 1.4785363674163818\nEpoch: 4 \tBatch: 68 \tLoss: 1.4613168239593506\nEpoch: 4 \tBatch: 69 \tLoss: 1.4685108661651611\nEpoch: 4 \tBatch: 70 \tLoss: 1.4694249629974365\nEpoch: 4 \tBatch: 71 \tLoss: 1.4728163480758667\nEpoch: 4 \tBatch: 72 \tLoss: 1.46309232711792\nEpoch: 4 \tBatch: 73 \tLoss: 1.4692015647888184\nEpoch: 4 \tBatch: 74 \tLoss: 1.479949712753296\nEpoch: 4 \tBatch: 75 \tLoss: 1.469700574874878\nEpoch: 4 \tBatch: 76 \tLoss: 1.4612596035003662\nEpoch: 4 \tBatch: 77 \tLoss: 1.4793035984039307\nEpoch: 4 \tBatch: 78 \tLoss: 1.4693228006362915\nEpoch: 4 \tBatch: 79 \tLoss: 1.4792675971984863\nEpoch: 4 \tBatch: 80 \tLoss: 1.4849215745925903\nEpoch: 4 \tBatch: 81 \tLoss: 1.4682481288909912\nEpoch: 4 \tBatch: 82 \tLoss: 1.4690018892288208\nEpoch: 4 \tBatch: 83 \tLoss: 1.4690673351287842\nEpoch: 4 \tBatch: 84 \tLoss: 1.4745638370513916\nEpoch: 4 \tBatch: 85 \tLoss: 1.47182297706604\nEpoch: 4 \tBatch: 86 \tLoss: 1.4708086252212524\nEpoch: 4 \tBatch: 87 \tLoss: 1.4619659185409546\nEpoch: 4 \tBatch: 88 \tLoss: 1.4774954319000244\nEpoch: 4 \tBatch: 89 \tLoss: 1.4803802967071533\nEpoch: 4 \tBatch: 90 \tLoss: 1.483511209487915\nEpoch: 4 \tBatch: 91 \tLoss: 1.4859808683395386\nEpoch: 4 \tBatch: 92 \tLoss: 1.4698293209075928\nEpoch: 4 \tBatch: 93 \tLoss: 1.4761143922805786\nEpoch: 4 \tBatch: 94 \tLoss: 1.4689388275146484\nEpoch: 4 \tBatch: 95 \tLoss: 1.4627180099487305\nEpoch: 4 \tBatch: 96 \tLoss: 1.4695606231689453\nEpoch: 4 \tBatch: 97 \tLoss: 1.4616076946258545\nEpoch: 4 \tBatch: 98 \tLoss: 1.469222068786621\nEpoch: 4 \tBatch: 99 \tLoss: 1.4659030437469482\nEpoch: 4 \tBatch: 100 \tLoss: 1.4774811267852783\nEpoch: 4 \tBatch: 101 \tLoss: 1.462805151939392\nEpoch: 4 \tBatch: 102 \tLoss: 1.5010868310928345\nEpoch: 4 \tBatch: 103 \tLoss: 1.4621175527572632\nEpoch: 4 \tBatch: 104 \tLoss: 1.4704639911651611\nEpoch: 4 \tBatch: 105 \tLoss: 1.477354645729065\nEpoch: 4 \tBatch: 106 \tLoss: 1.4693632125854492\nEpoch: 4 \tBatch: 107 \tLoss: 1.4661622047424316\nEpoch: 4 \tBatch: 108 \tLoss: 1.4770097732543945\nEpoch: 4 \tBatch: 109 \tLoss: 1.4723104238510132\nEpoch: 4 \tBatch: 110 \tLoss: 1.4674592018127441\nEpoch: 4 \tBatch: 111 \tLoss: 1.46139395236969\nEpoch: 4 \tBatch: 112 \tLoss: 1.4811064004898071\nEpoch: 4 \tBatch: 113 \tLoss: 1.486756443977356\nEpoch: 4 \tBatch: 114 \tLoss: 1.467606782913208\nEpoch: 4 \tBatch: 115 \tLoss: 1.4676512479782104\nEpoch: 4 \tBatch: 116 \tLoss: 1.4773355722427368\nEpoch: 4 \tBatch: 117 \tLoss: 1.4760768413543701\nEpoch: 4 \tBatch: 118 \tLoss: 1.4775700569152832\nEpoch: 4 \tBatch: 119 \tLoss: 1.4771324396133423\nEpoch: 4 \tBatch: 120 \tLoss: 1.4685113430023193\nEpoch: 4 \tBatch: 121 \tLoss: 1.468449592590332\nEpoch: 4 \tBatch: 122 \tLoss: 1.4780380725860596\nEpoch: 4 \tBatch: 123 \tLoss: 1.489159107208252\nEpoch: 4 \tBatch: 124 \tLoss: 1.4674110412597656\nEpoch: 4 \tBatch: 125 \tLoss: 1.4842530488967896\nEpoch: 4 \tBatch: 126 \tLoss: 1.471152663230896\nEpoch: 4 \tBatch: 127 \tLoss: 1.469123363494873\nEpoch: 4 \tBatch: 128 \tLoss: 1.4997090101242065\nEpoch: 4 \tBatch: 129 \tLoss: 1.4694700241088867\nEpoch: 4 \tBatch: 130 \tLoss: 1.472432255744934\nEpoch: 4 \tBatch: 131 \tLoss: 1.4623942375183105\nEpoch: 4 \tBatch: 132 \tLoss: 1.4697339534759521\nEpoch: 4 \tBatch: 133 \tLoss: 1.461787462234497\nEpoch: 4 \tBatch: 134 \tLoss: 1.4769164323806763\nEpoch: 4 \tBatch: 135 \tLoss: 1.461242914199829\nEpoch: 4 \tBatch: 136 \tLoss: 1.470884919166565\nEpoch: 4 \tBatch: 137 \tLoss: 1.4617632627487183\nEpoch: 4 \tBatch: 138 \tLoss: 1.4739816188812256\nEpoch: 4 \tBatch: 139 \tLoss: 1.4773482084274292\nEpoch: 4 \tBatch: 140 \tLoss: 1.4779232740402222\nEpoch: 4 \tBatch: 141 \tLoss: 1.47685968875885\nEpoch: 4 \tBatch: 142 \tLoss: 1.4892579317092896\nEpoch: 4 \tBatch: 143 \tLoss: 1.4767873287200928\nEpoch: 4 \tBatch: 144 \tLoss: 1.4769827127456665\nEpoch: 4 \tBatch: 145 \tLoss: 1.4620029926300049\nEpoch: 4 \tBatch: 146 \tLoss: 1.4622459411621094\nEpoch: 4 \tBatch: 147 \tLoss: 1.4621855020523071\nEpoch: 4 \tBatch: 148 \tLoss: 1.4611531496047974\nEpoch: 4 \tBatch: 149 \tLoss: 1.4842525720596313\nEpoch: 4 \tBatch: 150 \tLoss: 1.48173987865448\nEpoch: 4 \tBatch: 151 \tLoss: 1.473522424697876\nEpoch: 4 \tBatch: 152 \tLoss: 1.464881420135498\nEpoch: 4 \tBatch: 153 \tLoss: 1.4684418439865112\nEpoch: 4 \tBatch: 154 \tLoss: 1.48698890209198\nEpoch: 4 \tBatch: 155 \tLoss: 1.4890031814575195\nEpoch: 4 \tBatch: 156 \tLoss: 1.4691966772079468\nEpoch: 4 \tBatch: 157 \tLoss: 1.4693423509597778\nEpoch: 4 \tBatch: 158 \tLoss: 1.492142677307129\nEpoch: 4 \tBatch: 159 \tLoss: 1.4642558097839355\nEpoch: 4 \tBatch: 160 \tLoss: 1.4643359184265137\nEpoch: 4 \tBatch: 161 \tLoss: 1.4702855348587036\nEpoch: 4 \tBatch: 162 \tLoss: 1.4722546339035034\nEpoch: 4 \tBatch: 163 \tLoss: 1.4688868522644043\nEpoch: 4 \tBatch: 164 \tLoss: 1.4613252878189087\nEpoch: 4 \tBatch: 165 \tLoss: 1.4695382118225098\nEpoch: 4 \tBatch: 166 \tLoss: 1.497280240058899\nEpoch: 4 \tBatch: 167 \tLoss: 1.4615024328231812\nEpoch: 4 \tBatch: 168 \tLoss: 1.4612292051315308\nEpoch: 4 \tBatch: 169 \tLoss: 1.4715615510940552\nEpoch: 4 \tBatch: 170 \tLoss: 1.4767059087753296\nEpoch: 4 \tBatch: 171 \tLoss: 1.4694799184799194\nEpoch: 4 \tBatch: 172 \tLoss: 1.465155005455017\nEpoch: 4 \tBatch: 173 \tLoss: 1.465867280960083\nEpoch: 4 \tBatch: 174 \tLoss: 1.4705058336257935\nEpoch: 4 \tBatch: 175 \tLoss: 1.4690788984298706\nEpoch: 4 \tBatch: 176 \tLoss: 1.4613240957260132\nEpoch: 4 \tBatch: 177 \tLoss: 1.4841556549072266\nEpoch: 4 \tBatch: 178 \tLoss: 1.4644156694412231\nEpoch: 4 \tBatch: 179 \tLoss: 1.4721194505691528\nEpoch: 4 \tBatch: 180 \tLoss: 1.47622549533844\nEpoch: 4 \tBatch: 181 \tLoss: 1.4690901041030884\nEpoch: 4 \tBatch: 182 \tLoss: 1.4619232416152954\nEpoch: 4 \tBatch: 183 \tLoss: 1.4691555500030518\nEpoch: 4 \tBatch: 184 \tLoss: 1.4945006370544434\nEpoch: 4 \tBatch: 185 \tLoss: 1.4787330627441406\nEpoch: 4 \tBatch: 186 \tLoss: 1.462217092514038\nEpoch: 4 \tBatch: 187 \tLoss: 1.4761205911636353\nEpoch: 4 \tBatch: 188 \tLoss: 1.4632130861282349\nEpoch: 4 \tBatch: 189 \tLoss: 1.4611743688583374\nEpoch: 4 \tBatch: 190 \tLoss: 1.4765255451202393\nEpoch: 4 \tBatch: 191 \tLoss: 1.4613041877746582\nEpoch: 4 \tBatch: 192 \tLoss: 1.4923369884490967\nEpoch: 4 \tBatch: 193 \tLoss: 1.4665378332138062\nEpoch: 4 \tBatch: 194 \tLoss: 1.4689675569534302\nEpoch: 4 \tBatch: 195 \tLoss: 1.4717352390289307\nEpoch: 4 \tBatch: 196 \tLoss: 1.468907117843628\nEpoch: 4 \tBatch: 197 \tLoss: 1.4611769914627075\nEpoch: 4 \tBatch: 198 \tLoss: 1.4611512422561646\nEpoch: 4 \tBatch: 199 \tLoss: 1.4613571166992188\nEpoch: 4 \tBatch: 200 \tLoss: 1.461219310760498\nEpoch: 4 \tBatch: 201 \tLoss: 1.4762026071548462\nEpoch: 4 \tBatch: 202 \tLoss: 1.469177007675171\nEpoch: 4 \tBatch: 203 \tLoss: 1.4667623043060303\nEpoch: 4 \tBatch: 204 \tLoss: 1.4690934419631958\nEpoch: 4 \tBatch: 205 \tLoss: 1.4689911603927612\nEpoch: 4 \tBatch: 206 \tLoss: 1.4615137577056885\nEpoch: 4 \tBatch: 207 \tLoss: 1.4892746210098267\nEpoch: 4 \tBatch: 208 \tLoss: 1.4691786766052246\nEpoch: 4 \tBatch: 209 \tLoss: 1.469356656074524\nEpoch: 4 \tBatch: 210 \tLoss: 1.4615710973739624\nEpoch: 4 \tBatch: 211 \tLoss: 1.4689650535583496\nEpoch: 4 \tBatch: 212 \tLoss: 1.467437982559204\nEpoch: 4 \tBatch: 213 \tLoss: 1.4691240787506104\nEpoch: 4 \tBatch: 214 \tLoss: 1.461156964302063\nEpoch: 4 \tBatch: 215 \tLoss: 1.4736698865890503\nEpoch: 4 \tBatch: 216 \tLoss: 1.4846349954605103\nEpoch: 4 \tBatch: 217 \tLoss: 1.4617176055908203\nEpoch: 4 \tBatch: 218 \tLoss: 1.4627619981765747\nEpoch: 4 \tBatch: 219 \tLoss: 1.4621728658676147\nEpoch: 4 \tBatch: 220 \tLoss: 1.4618375301361084\nEpoch: 4 \tBatch: 221 \tLoss: 1.4759891033172607\nEpoch: 4 \tBatch: 222 \tLoss: 1.4689202308654785\nEpoch: 4 \tBatch: 223 \tLoss: 1.4676947593688965\nEpoch: 4 \tBatch: 224 \tLoss: 1.4690988063812256\nEpoch: 4 \tBatch: 225 \tLoss: 1.47231924533844\nEpoch: 4 \tBatch: 226 \tLoss: 1.4620475769042969\nEpoch: 4 \tBatch: 227 \tLoss: 1.4953114986419678\nEpoch: 4 \tBatch: 228 \tLoss: 1.4675370454788208\nEpoch: 4 \tBatch: 229 \tLoss: 1.461158275604248\nEpoch: 4 \tBatch: 230 \tLoss: 1.4611520767211914\nEpoch: 4 \tBatch: 231 \tLoss: 1.463156819343567\nEpoch: 4 \tBatch: 232 \tLoss: 1.476783275604248\nEpoch: 4 \tBatch: 233 \tLoss: 1.4612035751342773\nEpoch: 4 \tBatch: 234 \tLoss: 1.4690333604812622\nEpoch: 4 \tBatch: 235 \tLoss: 1.4799418449401855\nEpoch: 4 \tBatch: 236 \tLoss: 1.4643752574920654\nEpoch: 4 \tBatch: 237 \tLoss: 1.4639919996261597\nEpoch: 4 \tBatch: 238 \tLoss: 1.461279034614563\nEpoch: 4 \tBatch: 239 \tLoss: 1.468970775604248\nEpoch: 4 \tBatch: 240 \tLoss: 1.461498498916626\nEpoch: 4 \tBatch: 241 \tLoss: 1.4665480852127075\nEpoch: 4 \tBatch: 242 \tLoss: 1.4767025709152222\nEpoch: 4 \tBatch: 243 \tLoss: 1.4691259860992432\nEpoch: 4 \tBatch: 244 \tLoss: 1.4733442068099976\nEpoch: 4 \tBatch: 245 \tLoss: 1.4773855209350586\nEpoch: 4 \tBatch: 246 \tLoss: 1.4624037742614746\nEpoch: 4 \tBatch: 247 \tLoss: 1.4636276960372925\nEpoch: 4 \tBatch: 248 \tLoss: 1.4772310256958008\nEpoch: 4 \tBatch: 249 \tLoss: 1.4634255170822144\nEpoch: 4 \tBatch: 250 \tLoss: 1.4809635877609253\nEpoch: 4 \tBatch: 251 \tLoss: 1.4692387580871582\nEpoch: 4 \tBatch: 252 \tLoss: 1.4799522161483765\nEpoch: 4 \tBatch: 253 \tLoss: 1.4685442447662354\nEpoch: 4 \tBatch: 254 \tLoss: 1.4681220054626465\nEpoch: 4 \tBatch: 255 \tLoss: 1.483364462852478\nEpoch: 4 \tBatch: 256 \tLoss: 1.467991828918457\nEpoch: 4 \tBatch: 257 \tLoss: 1.469150424003601\nEpoch: 4 \tBatch: 258 \tLoss: 1.4843155145645142\nEpoch: 4 \tBatch: 259 \tLoss: 1.470763087272644\nEpoch: 4 \tBatch: 260 \tLoss: 1.471771001815796\nEpoch: 4 \tBatch: 261 \tLoss: 1.4611790180206299\nEpoch: 4 \tBatch: 262 \tLoss: 1.4842395782470703\nEpoch: 4 \tBatch: 263 \tLoss: 1.4829992055892944\nEpoch: 4 \tBatch: 264 \tLoss: 1.47947096824646\nEpoch: 4 \tBatch: 265 \tLoss: 1.482795000076294\nEpoch: 4 \tBatch: 266 \tLoss: 1.4847396612167358\nEpoch: 4 \tBatch: 267 \tLoss: 1.4613432884216309\nEpoch: 4 \tBatch: 268 \tLoss: 1.4664666652679443\nEpoch: 4 \tBatch: 269 \tLoss: 1.4686102867126465\nEpoch: 4 \tBatch: 270 \tLoss: 1.4615520238876343\nEpoch: 4 \tBatch: 271 \tLoss: 1.4704476594924927\nEpoch: 4 \tBatch: 272 \tLoss: 1.48435640335083\nEpoch: 4 \tBatch: 273 \tLoss: 1.4693117141723633\nEpoch: 4 \tBatch: 274 \tLoss: 1.4675071239471436\nEpoch: 4 \tBatch: 275 \tLoss: 1.4685324430465698\nEpoch: 4 \tBatch: 276 \tLoss: 1.4694327116012573\nEpoch: 4 \tBatch: 277 \tLoss: 1.4791834354400635\nEpoch: 4 \tBatch: 278 \tLoss: 1.4770764112472534\nEpoch: 4 \tBatch: 279 \tLoss: 1.4625591039657593\nEpoch: 4 \tBatch: 280 \tLoss: 1.4690533876419067\nEpoch: 4 \tBatch: 281 \tLoss: 1.4716287851333618\nEpoch: 4 \tBatch: 282 \tLoss: 1.4827488660812378\nEpoch: 4 \tBatch: 283 \tLoss: 1.4611577987670898\nEpoch: 4 \tBatch: 284 \tLoss: 1.4842593669891357\nEpoch: 4 \tBatch: 285 \tLoss: 1.4689908027648926\nEpoch: 4 \tBatch: 286 \tLoss: 1.4689773321151733\nEpoch: 4 \tBatch: 287 \tLoss: 1.462822675704956\nEpoch: 4 \tBatch: 288 \tLoss: 1.473617672920227\nEpoch: 4 \tBatch: 289 \tLoss: 1.4912011623382568\nEpoch: 4 \tBatch: 290 \tLoss: 1.4620788097381592\nEpoch: 4 \tBatch: 291 \tLoss: 1.4644203186035156\nEpoch: 4 \tBatch: 292 \tLoss: 1.475767731666565\nEpoch: 4 \tBatch: 293 \tLoss: 1.4706635475158691\nEpoch: 4 \tBatch: 294 \tLoss: 1.4701836109161377\nEpoch: 4 \tBatch: 295 \tLoss: 1.4677983522415161\nEpoch: 4 \tBatch: 296 \tLoss: 1.4612315893173218\nEpoch: 4 \tBatch: 297 \tLoss: 1.4940866231918335\nEpoch: 4 \tBatch: 298 \tLoss: 1.4611667394638062\nEpoch: 4 \tBatch: 299 \tLoss: 1.4633628129959106\nEpoch: 4 \tBatch: 300 \tLoss: 1.4620553255081177\nEpoch: 4 \tBatch: 301 \tLoss: 1.4855360984802246\nEpoch: 4 \tBatch: 302 \tLoss: 1.4665149450302124\nEpoch: 4 \tBatch: 303 \tLoss: 1.4688091278076172\nEpoch: 4 \tBatch: 304 \tLoss: 1.4643123149871826\nEpoch: 4 \tBatch: 305 \tLoss: 1.4853566884994507\nEpoch: 4 \tBatch: 306 \tLoss: 1.4611834287643433\nEpoch: 4 \tBatch: 307 \tLoss: 1.4696929454803467\nEpoch: 4 \tBatch: 308 \tLoss: 1.4769132137298584\nEpoch: 4 \tBatch: 309 \tLoss: 1.4745222330093384\nEpoch: 4 \tBatch: 310 \tLoss: 1.4780049324035645\nEpoch: 4 \tBatch: 311 \tLoss: 1.461251139640808\nEpoch: 4 \tBatch: 312 \tLoss: 1.4873777627944946\nEpoch: 4 \tBatch: 313 \tLoss: 1.4689643383026123\nEpoch: 4 \tBatch: 314 \tLoss: 1.4788435697555542\nEpoch: 4 \tBatch: 315 \tLoss: 1.468051791191101\nEpoch: 4 \tBatch: 316 \tLoss: 1.4616084098815918\nEpoch: 4 \tBatch: 317 \tLoss: 1.4611619710922241\nEpoch: 4 \tBatch: 318 \tLoss: 1.4712765216827393\nEpoch: 4 \tBatch: 319 \tLoss: 1.4616166353225708\nEpoch: 4 \tBatch: 320 \tLoss: 1.461341142654419\nEpoch: 4 \tBatch: 321 \tLoss: 1.467482328414917\nEpoch: 4 \tBatch: 322 \tLoss: 1.4689723253250122\nEpoch: 4 \tBatch: 323 \tLoss: 1.4647250175476074\nEpoch: 4 \tBatch: 324 \tLoss: 1.469618797302246\nEpoch: 4 \tBatch: 325 \tLoss: 1.4766874313354492\nEpoch: 4 \tBatch: 326 \tLoss: 1.461493968963623\nEpoch: 4 \tBatch: 327 \tLoss: 1.4716829061508179\nEpoch: 4 \tBatch: 328 \tLoss: 1.4623162746429443\nEpoch: 4 \tBatch: 329 \tLoss: 1.496612787246704\nEpoch: 4 \tBatch: 330 \tLoss: 1.479475975036621\nEpoch: 4 \tBatch: 331 \tLoss: 1.4691660404205322\nEpoch: 4 \tBatch: 332 \tLoss: 1.4687443971633911\nEpoch: 4 \tBatch: 333 \tLoss: 1.477345585823059\nEpoch: 4 \tBatch: 334 \tLoss: 1.4693275690078735\nEpoch: 4 \tBatch: 335 \tLoss: 1.4804340600967407\nEpoch: 4 \tBatch: 336 \tLoss: 1.4689698219299316\nEpoch: 4 \tBatch: 337 \tLoss: 1.47149658203125\nEpoch: 4 \tBatch: 338 \tLoss: 1.472541093826294\nEpoch: 4 \tBatch: 339 \tLoss: 1.4784947633743286\nEpoch: 4 \tBatch: 340 \tLoss: 1.4777172803878784\nEpoch: 4 \tBatch: 341 \tLoss: 1.4692388772964478\nEpoch: 4 \tBatch: 342 \tLoss: 1.4697353839874268\nEpoch: 4 \tBatch: 343 \tLoss: 1.4778484106063843\nEpoch: 4 \tBatch: 344 \tLoss: 1.4697940349578857\nEpoch: 4 \tBatch: 345 \tLoss: 1.4749778509140015\nEpoch: 4 \tBatch: 346 \tLoss: 1.4632304906845093\nEpoch: 4 \tBatch: 347 \tLoss: 1.4692015647888184\nEpoch: 4 \tBatch: 348 \tLoss: 1.4690388441085815\nEpoch: 4 \tBatch: 349 \tLoss: 1.4710893630981445\nEpoch: 4 \tBatch: 350 \tLoss: 1.469031572341919\nEpoch: 4 \tBatch: 351 \tLoss: 1.4611644744873047\nEpoch: 4 \tBatch: 352 \tLoss: 1.4632385969161987\nEpoch: 4 \tBatch: 353 \tLoss: 1.4615700244903564\nEpoch: 4 \tBatch: 354 \tLoss: 1.4858503341674805\nEpoch: 4 \tBatch: 355 \tLoss: 1.4613227844238281\nEpoch: 4 \tBatch: 356 \tLoss: 1.4612104892730713\nEpoch: 4 \tBatch: 357 \tLoss: 1.468969702720642\nEpoch: 4 \tBatch: 358 \tLoss: 1.4613193273544312\nEpoch: 4 \tBatch: 359 \tLoss: 1.467898964881897\nEpoch: 4 \tBatch: 360 \tLoss: 1.469333291053772\nEpoch: 4 \tBatch: 361 \tLoss: 1.468969464302063\nEpoch: 4 \tBatch: 362 \tLoss: 1.4939981698989868\nEpoch: 4 \tBatch: 363 \tLoss: 1.468963384628296\nEpoch: 4 \tBatch: 364 \tLoss: 1.4619218111038208\nEpoch: 4 \tBatch: 365 \tLoss: 1.4769625663757324\nEpoch: 4 \tBatch: 366 \tLoss: 1.4780281782150269\nEpoch: 4 \tBatch: 367 \tLoss: 1.477011799812317\nEpoch: 4 \tBatch: 368 \tLoss: 1.4685003757476807\nEpoch: 4 \tBatch: 369 \tLoss: 1.4893484115600586\nEpoch: 4 \tBatch: 370 \tLoss: 1.4617626667022705\nEpoch: 4 \tBatch: 371 \tLoss: 1.4616910219192505\nEpoch: 4 \tBatch: 372 \tLoss: 1.4690967798233032\nEpoch: 4 \tBatch: 373 \tLoss: 1.4624354839324951\nEpoch: 4 \tBatch: 374 \tLoss: 1.4689794778823853\nEpoch: 4 \tBatch: 375 \tLoss: 1.4612016677856445\nEpoch: 4 \tBatch: 376 \tLoss: 1.4767509698867798\nEpoch: 4 \tBatch: 377 \tLoss: 1.4635740518569946\nEpoch: 4 \tBatch: 378 \tLoss: 1.4718090295791626\nEpoch: 4 \tBatch: 379 \tLoss: 1.4688917398452759\nEpoch: 4 \tBatch: 380 \tLoss: 1.4612746238708496\nEpoch: 4 \tBatch: 381 \tLoss: 1.4860221147537231\nEpoch: 4 \tBatch: 382 \tLoss: 1.4834476709365845\nEpoch: 4 \tBatch: 383 \tLoss: 1.474955677986145\nEpoch: 4 \tBatch: 384 \tLoss: 1.4612358808517456\nEpoch: 4 \tBatch: 385 \tLoss: 1.4689176082611084\nEpoch: 4 \tBatch: 386 \tLoss: 1.470223307609558\nEpoch: 4 \tBatch: 387 \tLoss: 1.4611603021621704\nEpoch: 4 \tBatch: 388 \tLoss: 1.4769072532653809\nEpoch: 4 \tBatch: 389 \tLoss: 1.464343547821045\nEpoch: 4 \tBatch: 390 \tLoss: 1.4611542224884033\nTraining Complete. Final loss = 1.4611542224884033\nTrain Accuracy: 99.316\nValidation Accuracy: 98.54\nTest Accuracy: 98.63\n"
    }
   ],
   "source": [
    "#Set argument batch_norm to True\n",
    "cnn_with_bn = ConvNet(batch_norm=True).to(device)\n",
    "optimiser = torch.optim.Adam(cnn_with_bn.parameters(), lr=learning_rate)\n",
    "writer2 = SummaryWriter(log_dir=\"runs/cnn_bn_lr0005\")\n",
    "train(cnn_with_bn, epochs, writer=writer2)\n",
    "print('Train Accuracy:', calc_accuracy(cnn2, train_loader))\n",
    "print('Validation Accuracy:', calc_accuracy(cnn2, val_loader))\n",
    "print('Test Accuracy:', calc_accuracy(cnn2, test_loader))"
   ]
  },
  {
   "source": [
    "From the plot in tensorboard, you should see that the convergence happens a lot earlier. The model therefore seems to be more robust from the beginning. Next, you can try using a higher learing rate. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": ": 1.4697396755218506\nEpoch: 3 \tBatch: 355 \tLoss: 1.4726462364196777\nEpoch: 3 \tBatch: 356 \tLoss: 1.4646005630493164\nEpoch: 3 \tBatch: 357 \tLoss: 1.4698542356491089\nEpoch: 3 \tBatch: 358 \tLoss: 1.4690487384796143\nEpoch: 3 \tBatch: 359 \tLoss: 1.4615857601165771\nEpoch: 3 \tBatch: 360 \tLoss: 1.4766308069229126\nEpoch: 3 \tBatch: 361 \tLoss: 1.4773776531219482\nEpoch: 3 \tBatch: 362 \tLoss: 1.4771806001663208\nEpoch: 3 \tBatch: 363 \tLoss: 1.4713208675384521\nEpoch: 3 \tBatch: 364 \tLoss: 1.46147620677948\nEpoch: 3 \tBatch: 365 \tLoss: 1.4868123531341553\nEpoch: 3 \tBatch: 366 \tLoss: 1.465869426727295\nEpoch: 3 \tBatch: 367 \tLoss: 1.486151099205017\nEpoch: 3 \tBatch: 368 \tLoss: 1.4854943752288818\nEpoch: 3 \tBatch: 369 \tLoss: 1.469657301902771\nEpoch: 3 \tBatch: 370 \tLoss: 1.4883553981781006\nEpoch: 3 \tBatch: 371 \tLoss: 1.476812481880188\nEpoch: 3 \tBatch: 372 \tLoss: 1.4651477336883545\nEpoch: 3 \tBatch: 373 \tLoss: 1.4831621646881104\nEpoch: 3 \tBatch: 374 \tLoss: 1.474079966545105\nEpoch: 3 \tBatch: 375 \tLoss: 1.4694504737854004\nEpoch: 3 \tBatch: 376 \tLoss: 1.4689915180206299\nEpoch: 3 \tBatch: 377 \tLoss: 1.4628417491912842\nEpoch: 3 \tBatch: 378 \tLoss: 1.4721732139587402\nEpoch: 3 \tBatch: 379 \tLoss: 1.461173415184021\nEpoch: 3 \tBatch: 380 \tLoss: 1.476733684539795\nEpoch: 3 \tBatch: 381 \tLoss: 1.4778070449829102\nEpoch: 3 \tBatch: 382 \tLoss: 1.4703187942504883\nEpoch: 3 \tBatch: 383 \tLoss: 1.465504765510559\nEpoch: 3 \tBatch: 384 \tLoss: 1.4617125988006592\nEpoch: 3 \tBatch: 385 \tLoss: 1.4852380752563477\nEpoch: 3 \tBatch: 386 \tLoss: 1.4768805503845215\nEpoch: 3 \tBatch: 387 \tLoss: 1.4693593978881836\nEpoch: 3 \tBatch: 388 \tLoss: 1.4808423519134521\nEpoch: 3 \tBatch: 389 \tLoss: 1.4614760875701904\nEpoch: 3 \tBatch: 390 \tLoss: 1.4766286611557007\nEpoch: 4 \tBatch: 0 \tLoss: 1.4946300983428955\nEpoch: 4 \tBatch: 1 \tLoss: 1.4756815433502197\nEpoch: 4 \tBatch: 2 \tLoss: 1.4645198583602905\nEpoch: 4 \tBatch: 3 \tLoss: 1.4674358367919922\nEpoch: 4 \tBatch: 4 \tLoss: 1.4629038572311401\nEpoch: 4 \tBatch: 5 \tLoss: 1.4690780639648438\nEpoch: 4 \tBatch: 6 \tLoss: 1.4766216278076172\nEpoch: 4 \tBatch: 7 \tLoss: 1.4778242111206055\nEpoch: 4 \tBatch: 8 \tLoss: 1.4890940189361572\nEpoch: 4 \tBatch: 9 \tLoss: 1.468136191368103\nEpoch: 4 \tBatch: 10 \tLoss: 1.4751944541931152\nEpoch: 4 \tBatch: 11 \tLoss: 1.469606876373291\nEpoch: 4 \tBatch: 12 \tLoss: 1.465159296989441\nEpoch: 4 \tBatch: 13 \tLoss: 1.468285083770752\nEpoch: 4 \tBatch: 14 \tLoss: 1.467079758644104\nEpoch: 4 \tBatch: 15 \tLoss: 1.4649344682693481\nEpoch: 4 \tBatch: 16 \tLoss: 1.4621703624725342\nEpoch: 4 \tBatch: 17 \tLoss: 1.4741432666778564\nEpoch: 4 \tBatch: 18 \tLoss: 1.4723931550979614\nEpoch: 4 \tBatch: 19 \tLoss: 1.4666568040847778\nEpoch: 4 \tBatch: 20 \tLoss: 1.461679220199585\nEpoch: 4 \tBatch: 21 \tLoss: 1.4690485000610352\nEpoch: 4 \tBatch: 22 \tLoss: 1.4649487733840942\nEpoch: 4 \tBatch: 23 \tLoss: 1.4620484113693237\nEpoch: 4 \tBatch: 24 \tLoss: 1.4642565250396729\nEpoch: 4 \tBatch: 25 \tLoss: 1.4715123176574707\nEpoch: 4 \tBatch: 26 \tLoss: 1.4619181156158447\nEpoch: 4 \tBatch: 27 \tLoss: 1.4732171297073364\nEpoch: 4 \tBatch: 28 \tLoss: 1.4611576795578003\nEpoch: 4 \tBatch: 29 \tLoss: 1.472307562828064\nEpoch: 4 \tBatch: 30 \tLoss: 1.4675097465515137\nEpoch: 4 \tBatch: 31 \tLoss: 1.465914011001587\nEpoch: 4 \tBatch: 32 \tLoss: 1.4703510999679565\nEpoch: 4 \tBatch: 33 \tLoss: 1.4689505100250244\nEpoch: 4 \tBatch: 34 \tLoss: 1.4676121473312378\nEpoch: 4 \tBatch: 35 \tLoss: 1.4615750312805176\nEpoch: 4 \tBatch: 36 \tLoss: 1.4736576080322266\nEpoch: 4 \tBatch: 37 \tLoss: 1.4841638803482056\nEpoch: 4 \tBatch: 38 \tLoss: 1.4769474267959595\nEpoch: 4 \tBatch: 39 \tLoss: 1.4695994853973389\nEpoch: 4 \tBatch: 40 \tLoss: 1.4614397287368774\nEpoch: 4 \tBatch: 41 \tLoss: 1.4723126888275146\nEpoch: 4 \tBatch: 42 \tLoss: 1.4618678092956543\nEpoch: 4 \tBatch: 43 \tLoss: 1.466623067855835\nEpoch: 4 \tBatch: 44 \tLoss: 1.4924551248550415\nEpoch: 4 \tBatch: 45 \tLoss: 1.4651193618774414\nEpoch: 4 \tBatch: 46 \tLoss: 1.4689021110534668\nEpoch: 4 \tBatch: 47 \tLoss: 1.5004469156265259\nEpoch: 4 \tBatch: 48 \tLoss: 1.4699867963790894\nEpoch: 4 \tBatch: 49 \tLoss: 1.4754434823989868\nEpoch: 4 \tBatch: 50 \tLoss: 1.4743071794509888\nEpoch: 4 \tBatch: 51 \tLoss: 1.4915664196014404\nEpoch: 4 \tBatch: 52 \tLoss: 1.4863369464874268\nEpoch: 4 \tBatch: 53 \tLoss: 1.491308569908142\nEpoch: 4 \tBatch: 54 \tLoss: 1.4830331802368164\nEpoch: 4 \tBatch: 55 \tLoss: 1.4729843139648438\nEpoch: 4 \tBatch: 56 \tLoss: 1.4617193937301636\nEpoch: 4 \tBatch: 57 \tLoss: 1.490458607673645\nEpoch: 4 \tBatch: 58 \tLoss: 1.4612653255462646\nEpoch: 4 \tBatch: 59 \tLoss: 1.4635471105575562\nEpoch: 4 \tBatch: 60 \tLoss: 1.5005642175674438\nEpoch: 4 \tBatch: 61 \tLoss: 1.478166103363037\nEpoch: 4 \tBatch: 62 \tLoss: 1.4645384550094604\nEpoch: 4 \tBatch: 63 \tLoss: 1.4683789014816284\nEpoch: 4 \tBatch: 64 \tLoss: 1.4814995527267456\nEpoch: 4 \tBatch: 65 \tLoss: 1.4691424369812012\nEpoch: 4 \tBatch: 66 \tLoss: 1.4805750846862793\nEpoch: 4 \tBatch: 67 \tLoss: 1.469759464263916\nEpoch: 4 \tBatch: 68 \tLoss: 1.4624491930007935\nEpoch: 4 \tBatch: 69 \tLoss: 1.4693939685821533\nEpoch: 4 \tBatch: 70 \tLoss: 1.4723167419433594\nEpoch: 4 \tBatch: 71 \tLoss: 1.4689658880233765\nEpoch: 4 \tBatch: 72 \tLoss: 1.4615838527679443\nEpoch: 4 \tBatch: 73 \tLoss: 1.463057279586792\nEpoch: 4 \tBatch: 74 \tLoss: 1.468964695930481\nEpoch: 4 \tBatch: 75 \tLoss: 1.462422490119934\nEpoch: 4 \tBatch: 76 \tLoss: 1.46245276927948\nEpoch: 4 \tBatch: 77 \tLoss: 1.4696124792099\nEpoch: 4 \tBatch: 78 \tLoss: 1.4757152795791626\nEpoch: 4 \tBatch: 79 \tLoss: 1.462104082107544\nEpoch: 4 \tBatch: 80 \tLoss: 1.4766284227371216\nEpoch: 4 \tBatch: 81 \tLoss: 1.4674217700958252\nEpoch: 4 \tBatch: 82 \tLoss: 1.4787399768829346\nEpoch: 4 \tBatch: 83 \tLoss: 1.4824423789978027\nEpoch: 4 \tBatch: 84 \tLoss: 1.4921530485153198\nEpoch: 4 \tBatch: 85 \tLoss: 1.466067910194397\nEpoch: 4 \tBatch: 86 \tLoss: 1.4774409532546997\nEpoch: 4 \tBatch: 87 \tLoss: 1.475122332572937\nEpoch: 4 \tBatch: 88 \tLoss: 1.4615236520767212\nEpoch: 4 \tBatch: 89 \tLoss: 1.4736579656600952\nEpoch: 4 \tBatch: 90 \tLoss: 1.4716259241104126\nEpoch: 4 \tBatch: 91 \tLoss: 1.4621551036834717\nEpoch: 4 \tBatch: 92 \tLoss: 1.478488802909851\nEpoch: 4 \tBatch: 93 \tLoss: 1.4612808227539062\nEpoch: 4 \tBatch: 94 \tLoss: 1.4703700542449951\nEpoch: 4 \tBatch: 95 \tLoss: 1.470157504081726\nEpoch: 4 \tBatch: 96 \tLoss: 1.462664008140564\nEpoch: 4 \tBatch: 97 \tLoss: 1.4611929655075073\nEpoch: 4 \tBatch: 98 \tLoss: 1.4704612493515015\nEpoch: 4 \tBatch: 99 \tLoss: 1.4937384128570557\nEpoch: 4 \tBatch: 100 \tLoss: 1.4694437980651855\nEpoch: 4 \tBatch: 101 \tLoss: 1.4685693979263306\nEpoch: 4 \tBatch: 102 \tLoss: 1.4734524488449097\nEpoch: 4 \tBatch: 103 \tLoss: 1.4631603956222534\nEpoch: 4 \tBatch: 104 \tLoss: 1.4658269882202148\nEpoch: 4 \tBatch: 105 \tLoss: 1.4699631929397583\nEpoch: 4 \tBatch: 106 \tLoss: 1.470495581626892\nEpoch: 4 \tBatch: 107 \tLoss: 1.4614611864089966\nEpoch: 4 \tBatch: 108 \tLoss: 1.4613476991653442\nEpoch: 4 \tBatch: 109 \tLoss: 1.4770721197128296\nEpoch: 4 \tBatch: 110 \tLoss: 1.477483868598938\nEpoch: 4 \tBatch: 111 \tLoss: 1.4765491485595703\nEpoch: 4 \tBatch: 112 \tLoss: 1.47175133228302\nEpoch: 4 \tBatch: 113 \tLoss: 1.469126582145691\nEpoch: 4 \tBatch: 114 \tLoss: 1.481077790260315\nEpoch: 4 \tBatch: 115 \tLoss: 1.4964804649353027\nEpoch: 4 \tBatch: 116 \tLoss: 1.4760899543762207\nEpoch: 4 \tBatch: 117 \tLoss: 1.4692126512527466\nEpoch: 4 \tBatch: 118 \tLoss: 1.4769296646118164\nEpoch: 4 \tBatch: 119 \tLoss: 1.4643590450286865\nEpoch: 4 \tBatch: 120 \tLoss: 1.491190791130066\nEpoch: 4 \tBatch: 121 \tLoss: 1.470069408416748\nEpoch: 4 \tBatch: 122 \tLoss: 1.4848525524139404\nEpoch: 4 \tBatch: 123 \tLoss: 1.4920859336853027\nEpoch: 4 \tBatch: 124 \tLoss: 1.4750339984893799\nEpoch: 4 \tBatch: 125 \tLoss: 1.477143406867981\nEpoch: 4 \tBatch: 126 \tLoss: 1.4702956676483154\nEpoch: 4 \tBatch: 127 \tLoss: 1.4852519035339355\nEpoch: 4 \tBatch: 128 \tLoss: 1.4730840921401978\nEpoch: 4 \tBatch: 129 \tLoss: 1.4715511798858643\nEpoch: 4 \tBatch: 130 \tLoss: 1.4795337915420532\nEpoch: 4 \tBatch: 131 \tLoss: 1.4892797470092773\nEpoch: 4 \tBatch: 132 \tLoss: 1.481529951095581\nEpoch: 4 \tBatch: 133 \tLoss: 1.4639486074447632\nEpoch: 4 \tBatch: 134 \tLoss: 1.4706840515136719\nEpoch: 4 \tBatch: 135 \tLoss: 1.4615579843521118\nEpoch: 4 \tBatch: 136 \tLoss: 1.4688224792480469\nEpoch: 4 \tBatch: 137 \tLoss: 1.47953462600708\nEpoch: 4 \tBatch: 138 \tLoss: 1.4620798826217651\nEpoch: 4 \tBatch: 139 \tLoss: 1.4710348844528198\nEpoch: 4 \tBatch: 140 \tLoss: 1.4627418518066406\nEpoch: 4 \tBatch: 141 \tLoss: 1.4720693826675415\nEpoch: 4 \tBatch: 142 \tLoss: 1.4690724611282349\nEpoch: 4 \tBatch: 143 \tLoss: 1.4689900875091553\nEpoch: 4 \tBatch: 144 \tLoss: 1.470324993133545\nEpoch: 4 \tBatch: 145 \tLoss: 1.4684607982635498\nEpoch: 4 \tBatch: 146 \tLoss: 1.4613022804260254\nEpoch: 4 \tBatch: 147 \tLoss: 1.4719547033309937\nEpoch: 4 \tBatch: 148 \tLoss: 1.4658334255218506\nEpoch: 4 \tBatch: 149 \tLoss: 1.4636774063110352\nEpoch: 4 \tBatch: 150 \tLoss: 1.4798966646194458\nEpoch: 4 \tBatch: 151 \tLoss: 1.4825323820114136\nEpoch: 4 \tBatch: 152 \tLoss: 1.4867528676986694\nEpoch: 4 \tBatch: 153 \tLoss: 1.4698596000671387\nEpoch: 4 \tBatch: 154 \tLoss: 1.4696614742279053\nEpoch: 4 \tBatch: 155 \tLoss: 1.4689159393310547\nEpoch: 4 \tBatch: 156 \tLoss: 1.4771995544433594\nEpoch: 4 \tBatch: 157 \tLoss: 1.4851371049880981\nEpoch: 4 \tBatch: 158 \tLoss: 1.4621152877807617\nEpoch: 4 \tBatch: 159 \tLoss: 1.4613591432571411\nEpoch: 4 \tBatch: 160 \tLoss: 1.462798833847046\nEpoch: 4 \tBatch: 161 \tLoss: 1.4638550281524658\nEpoch: 4 \tBatch: 162 \tLoss: 1.4920271635055542\nEpoch: 4 \tBatch: 163 \tLoss: 1.469355821609497\nEpoch: 4 \tBatch: 164 \tLoss: 1.4767658710479736\nEpoch: 4 \tBatch: 165 \tLoss: 1.4977415800094604\nEpoch: 4 \tBatch: 166 \tLoss: 1.4624764919281006\nEpoch: 4 \tBatch: 167 \tLoss: 1.4842184782028198\nEpoch: 4 \tBatch: 168 \tLoss: 1.4615224599838257\nEpoch: 4 \tBatch: 169 \tLoss: 1.469079613685608\nEpoch: 4 \tBatch: 170 \tLoss: 1.4728175401687622\nEpoch: 4 \tBatch: 171 \tLoss: 1.461916208267212\nEpoch: 4 \tBatch: 172 \tLoss: 1.47079336643219\nEpoch: 4 \tBatch: 173 \tLoss: 1.4697717428207397\nEpoch: 4 \tBatch: 174 \tLoss: 1.4621227979660034\nEpoch: 4 \tBatch: 175 \tLoss: 1.4768599271774292\nEpoch: 4 \tBatch: 176 \tLoss: 1.4675734043121338\nEpoch: 4 \tBatch: 177 \tLoss: 1.4699300527572632\nEpoch: 4 \tBatch: 178 \tLoss: 1.4613020420074463\nEpoch: 4 \tBatch: 179 \tLoss: 1.4784637689590454\nEpoch: 4 \tBatch: 180 \tLoss: 1.474158763885498\nEpoch: 4 \tBatch: 181 \tLoss: 1.4843240976333618\nEpoch: 4 \tBatch: 182 \tLoss: 1.4692245721817017\nEpoch: 4 \tBatch: 183 \tLoss: 1.491302728652954\nEpoch: 4 \tBatch: 184 \tLoss: 1.4767985343933105\nEpoch: 4 \tBatch: 185 \tLoss: 1.4758381843566895\nEpoch: 4 \tBatch: 186 \tLoss: 1.471017837524414\nEpoch: 4 \tBatch: 187 \tLoss: 1.4694952964782715\nEpoch: 4 \tBatch: 188 \tLoss: 1.4611735343933105\nEpoch: 4 \tBatch: 189 \tLoss: 1.4713221788406372\nEpoch: 4 \tBatch: 190 \tLoss: 1.4622604846954346\nEpoch: 4 \tBatch: 191 \tLoss: 1.4691247940063477\nEpoch: 4 \tBatch: 192 \tLoss: 1.4717767238616943\nEpoch: 4 \tBatch: 193 \tLoss: 1.4714409112930298\nEpoch: 4 \tBatch: 194 \tLoss: 1.4806828498840332\nEpoch: 4 \tBatch: 195 \tLoss: 1.4676918983459473\nEpoch: 4 \tBatch: 196 \tLoss: 1.4767311811447144\nEpoch: 4 \tBatch: 197 \tLoss: 1.4789942502975464\nEpoch: 4 \tBatch: 198 \tLoss: 1.47075617313385\nEpoch: 4 \tBatch: 199 \tLoss: 1.497684121131897\nEpoch: 4 \tBatch: 200 \tLoss: 1.4704164266586304\nEpoch: 4 \tBatch: 201 \tLoss: 1.4767225980758667\nEpoch: 4 \tBatch: 202 \tLoss: 1.464243769645691\nEpoch: 4 \tBatch: 203 \tLoss: 1.471469521522522\nEpoch: 4 \tBatch: 204 \tLoss: 1.4719852209091187\nEpoch: 4 \tBatch: 205 \tLoss: 1.4624147415161133\nEpoch: 4 \tBatch: 206 \tLoss: 1.4747464656829834\nEpoch: 4 \tBatch: 207 \tLoss: 1.4776474237442017\nEpoch: 4 \tBatch: 208 \tLoss: 1.4851768016815186\nEpoch: 4 \tBatch: 209 \tLoss: 1.4622148275375366\nEpoch: 4 \tBatch: 210 \tLoss: 1.468940019607544\nEpoch: 4 \tBatch: 211 \tLoss: 1.461174726486206\nEpoch: 4 \tBatch: 212 \tLoss: 1.461320161819458\nEpoch: 4 \tBatch: 213 \tLoss: 1.4621045589447021\nEpoch: 4 \tBatch: 214 \tLoss: 1.4693686962127686\nEpoch: 4 \tBatch: 215 \tLoss: 1.4686615467071533\nEpoch: 4 \tBatch: 216 \tLoss: 1.4844213724136353\nEpoch: 4 \tBatch: 217 \tLoss: 1.4796925783157349\nEpoch: 4 \tBatch: 218 \tLoss: 1.474039077758789\nEpoch: 4 \tBatch: 219 \tLoss: 1.4611914157867432\nEpoch: 4 \tBatch: 220 \tLoss: 1.4696812629699707\nEpoch: 4 \tBatch: 221 \tLoss: 1.4929639101028442\nEpoch: 4 \tBatch: 222 \tLoss: 1.476388692855835\nEpoch: 4 \tBatch: 223 \tLoss: 1.4723684787750244\nEpoch: 4 \tBatch: 224 \tLoss: 1.4681692123413086\nEpoch: 4 \tBatch: 225 \tLoss: 1.496944546699524\nEpoch: 4 \tBatch: 226 \tLoss: 1.4848092794418335\nEpoch: 4 \tBatch: 227 \tLoss: 1.4612597227096558\nEpoch: 4 \tBatch: 228 \tLoss: 1.4775480031967163\nEpoch: 4 \tBatch: 229 \tLoss: 1.474155068397522\nEpoch: 4 \tBatch: 230 \tLoss: 1.4670315980911255\nEpoch: 4 \tBatch: 231 \tLoss: 1.4746856689453125\nEpoch: 4 \tBatch: 232 \tLoss: 1.4841313362121582\nEpoch: 4 \tBatch: 233 \tLoss: 1.4694631099700928\nEpoch: 4 \tBatch: 234 \tLoss: 1.4637442827224731\nEpoch: 4 \tBatch: 235 \tLoss: 1.4615892171859741\nEpoch: 4 \tBatch: 236 \tLoss: 1.4773542881011963\nEpoch: 4 \tBatch: 237 \tLoss: 1.461937665939331\nEpoch: 4 \tBatch: 238 \tLoss: 1.4613093137741089\nEpoch: 4 \tBatch: 239 \tLoss: 1.4690783023834229\nEpoch: 4 \tBatch: 240 \tLoss: 1.4733270406723022\nEpoch: 4 \tBatch: 241 \tLoss: 1.4710347652435303\nEpoch: 4 \tBatch: 242 \tLoss: 1.491091251373291\nEpoch: 4 \tBatch: 243 \tLoss: 1.4614441394805908\nEpoch: 4 \tBatch: 244 \tLoss: 1.4768562316894531\nEpoch: 4 \tBatch: 245 \tLoss: 1.470885992050171\nEpoch: 4 \tBatch: 246 \tLoss: 1.4783707857131958\nEpoch: 4 \tBatch: 247 \tLoss: 1.4995017051696777\nEpoch: 4 \tBatch: 248 \tLoss: 1.467012643814087\nEpoch: 4 \tBatch: 249 \tLoss: 1.4703036546707153\nEpoch: 4 \tBatch: 250 \tLoss: 1.4701862335205078\nEpoch: 4 \tBatch: 251 \tLoss: 1.481945276260376\nEpoch: 4 \tBatch: 252 \tLoss: 1.4864252805709839\nEpoch: 4 \tBatch: 253 \tLoss: 1.4614551067352295\nEpoch: 4 \tBatch: 254 \tLoss: 1.4612644910812378\nEpoch: 4 \tBatch: 255 \tLoss: 1.4652644395828247\nEpoch: 4 \tBatch: 256 \tLoss: 1.476948857307434\nEpoch: 4 \tBatch: 257 \tLoss: 1.4780844449996948\nEpoch: 4 \tBatch: 258 \tLoss: 1.4611949920654297\nEpoch: 4 \tBatch: 259 \tLoss: 1.4690368175506592\nEpoch: 4 \tBatch: 260 \tLoss: 1.46897554397583\nEpoch: 4 \tBatch: 261 \tLoss: 1.4700589179992676\nEpoch: 4 \tBatch: 262 \tLoss: 1.4692139625549316\nEpoch: 4 \tBatch: 263 \tLoss: 1.4730324745178223\nEpoch: 4 \tBatch: 264 \tLoss: 1.4632325172424316\nEpoch: 4 \tBatch: 265 \tLoss: 1.4691565036773682\nEpoch: 4 \tBatch: 266 \tLoss: 1.479114294052124\nEpoch: 4 \tBatch: 267 \tLoss: 1.4649431705474854\nEpoch: 4 \tBatch: 268 \tLoss: 1.4878443479537964\nEpoch: 4 \tBatch: 269 \tLoss: 1.4707210063934326\nEpoch: 4 \tBatch: 270 \tLoss: 1.4666680097579956\nEpoch: 4 \tBatch: 271 \tLoss: 1.4693313837051392\nEpoch: 4 \tBatch: 272 \tLoss: 1.4748170375823975\nEpoch: 4 \tBatch: 273 \tLoss: 1.4822359085083008\nEpoch: 4 \tBatch: 274 \tLoss: 1.4789178371429443\nEpoch: 4 \tBatch: 275 \tLoss: 1.4843634366989136\nEpoch: 4 \tBatch: 276 \tLoss: 1.4774588346481323\nEpoch: 4 \tBatch: 277 \tLoss: 1.4694089889526367\nEpoch: 4 \tBatch: 278 \tLoss: 1.4692049026489258\nEpoch: 4 \tBatch: 279 \tLoss: 1.4916034936904907\nEpoch: 4 \tBatch: 280 \tLoss: 1.4778209924697876\nEpoch: 4 \tBatch: 281 \tLoss: 1.4725563526153564\nEpoch: 4 \tBatch: 282 \tLoss: 1.4625009298324585\nEpoch: 4 \tBatch: 283 \tLoss: 1.4686975479125977\nEpoch: 4 \tBatch: 284 \tLoss: 1.4690135717391968\nEpoch: 4 \tBatch: 285 \tLoss: 1.462493896484375\nEpoch: 4 \tBatch: 286 \tLoss: 1.4692192077636719\nEpoch: 4 \tBatch: 287 \tLoss: 1.4654830694198608\nEpoch: 4 \tBatch: 288 \tLoss: 1.4623026847839355\nEpoch: 4 \tBatch: 289 \tLoss: 1.475851058959961\nEpoch: 4 \tBatch: 290 \tLoss: 1.4745228290557861\nEpoch: 4 \tBatch: 291 \tLoss: 1.4744304418563843\nEpoch: 4 \tBatch: 292 \tLoss: 1.4811125993728638\nEpoch: 4 \tBatch: 293 \tLoss: 1.4845787286758423\nEpoch: 4 \tBatch: 294 \tLoss: 1.4824994802474976\nEpoch: 4 \tBatch: 295 \tLoss: 1.4828091859817505\nEpoch: 4 \tBatch: 296 \tLoss: 1.4780546426773071\nEpoch: 4 \tBatch: 297 \tLoss: 1.4830498695373535\nEpoch: 4 \tBatch: 298 \tLoss: 1.470796823501587\nEpoch: 4 \tBatch: 299 \tLoss: 1.4689363241195679\nEpoch: 4 \tBatch: 300 \tLoss: 1.4633227586746216\nEpoch: 4 \tBatch: 301 \tLoss: 1.4613579511642456\nEpoch: 4 \tBatch: 302 \tLoss: 1.4615713357925415\nEpoch: 4 \tBatch: 303 \tLoss: 1.4734247922897339\nEpoch: 4 \tBatch: 304 \tLoss: 1.4611884355545044\nEpoch: 4 \tBatch: 305 \tLoss: 1.4938480854034424\nEpoch: 4 \tBatch: 306 \tLoss: 1.469030499458313\nEpoch: 4 \tBatch: 307 \tLoss: 1.4676713943481445\nEpoch: 4 \tBatch: 308 \tLoss: 1.4774563312530518\nEpoch: 4 \tBatch: 309 \tLoss: 1.4868699312210083\nEpoch: 4 \tBatch: 310 \tLoss: 1.4692192077636719\nEpoch: 4 \tBatch: 311 \tLoss: 1.4611564874649048\nEpoch: 4 \tBatch: 312 \tLoss: 1.4765565395355225\nEpoch: 4 \tBatch: 313 \tLoss: 1.4768071174621582\nEpoch: 4 \tBatch: 314 \tLoss: 1.4619450569152832\nEpoch: 4 \tBatch: 315 \tLoss: 1.477746844291687\nEpoch: 4 \tBatch: 316 \tLoss: 1.4756678342819214\nEpoch: 4 \tBatch: 317 \tLoss: 1.4612752199172974\nEpoch: 4 \tBatch: 318 \tLoss: 1.471091866493225\nEpoch: 4 \tBatch: 319 \tLoss: 1.4860750436782837\nEpoch: 4 \tBatch: 320 \tLoss: 1.4689651727676392\nEpoch: 4 \tBatch: 321 \tLoss: 1.4745646715164185\nEpoch: 4 \tBatch: 322 \tLoss: 1.4784467220306396\nEpoch: 4 \tBatch: 323 \tLoss: 1.4655566215515137\nEpoch: 4 \tBatch: 324 \tLoss: 1.4611902236938477\nEpoch: 4 \tBatch: 325 \tLoss: 1.4691250324249268\nEpoch: 4 \tBatch: 326 \tLoss: 1.4612150192260742\nEpoch: 4 \tBatch: 327 \tLoss: 1.471775770187378\nEpoch: 4 \tBatch: 328 \tLoss: 1.4803353548049927\nEpoch: 4 \tBatch: 329 \tLoss: 1.47672700881958\nEpoch: 4 \tBatch: 330 \tLoss: 1.4857149124145508\nEpoch: 4 \tBatch: 331 \tLoss: 1.4620527029037476\nEpoch: 4 \tBatch: 332 \tLoss: 1.5051257610321045\nEpoch: 4 \tBatch: 333 \tLoss: 1.4667311906814575\nEpoch: 4 \tBatch: 334 \tLoss: 1.4636815786361694\nEpoch: 4 \tBatch: 335 \tLoss: 1.463057279586792\nEpoch: 4 \tBatch: 336 \tLoss: 1.4671447277069092\nEpoch: 4 \tBatch: 337 \tLoss: 1.478186011314392\nEpoch: 4 \tBatch: 338 \tLoss: 1.4751023054122925\nEpoch: 4 \tBatch: 339 \tLoss: 1.4640201330184937\nEpoch: 4 \tBatch: 340 \tLoss: 1.4694154262542725\nEpoch: 4 \tBatch: 341 \tLoss: 1.4611955881118774\nEpoch: 4 \tBatch: 342 \tLoss: 1.4611583948135376\nEpoch: 4 \tBatch: 343 \tLoss: 1.4709354639053345\nEpoch: 4 \tBatch: 344 \tLoss: 1.4696625471115112\nEpoch: 4 \tBatch: 345 \tLoss: 1.4846922159194946\nEpoch: 4 \tBatch: 346 \tLoss: 1.4664051532745361\nEpoch: 4 \tBatch: 347 \tLoss: 1.476877212524414\nEpoch: 4 \tBatch: 348 \tLoss: 1.4689741134643555\nEpoch: 4 \tBatch: 349 \tLoss: 1.461700677871704\nEpoch: 4 \tBatch: 350 \tLoss: 1.4845479726791382\nEpoch: 4 \tBatch: 351 \tLoss: 1.4768383502960205\nEpoch: 4 \tBatch: 352 \tLoss: 1.4722596406936646\nEpoch: 4 \tBatch: 353 \tLoss: 1.4697840213775635\nEpoch: 4 \tBatch: 354 \tLoss: 1.47409987449646\nEpoch: 4 \tBatch: 355 \tLoss: 1.4769670963287354\nEpoch: 4 \tBatch: 356 \tLoss: 1.4700697660446167\nEpoch: 4 \tBatch: 357 \tLoss: 1.464060664176941\nEpoch: 4 \tBatch: 358 \tLoss: 1.4688974618911743\nEpoch: 4 \tBatch: 359 \tLoss: 1.4611961841583252\nEpoch: 4 \tBatch: 360 \tLoss: 1.4713821411132812\nEpoch: 4 \tBatch: 361 \tLoss: 1.4617693424224854\nEpoch: 4 \tBatch: 362 \tLoss: 1.464396595954895\nEpoch: 4 \tBatch: 363 \tLoss: 1.4768221378326416\nEpoch: 4 \tBatch: 364 \tLoss: 1.4612209796905518\nEpoch: 4 \tBatch: 365 \tLoss: 1.4698599576950073\nEpoch: 4 \tBatch: 366 \tLoss: 1.462604284286499\nEpoch: 4 \tBatch: 367 \tLoss: 1.4846714735031128\nEpoch: 4 \tBatch: 368 \tLoss: 1.4677249193191528\nEpoch: 4 \tBatch: 369 \tLoss: 1.4801079034805298\nEpoch: 4 \tBatch: 370 \tLoss: 1.467362642288208\nEpoch: 4 \tBatch: 371 \tLoss: 1.4613609313964844\nEpoch: 4 \tBatch: 372 \tLoss: 1.4754588603973389\nEpoch: 4 \tBatch: 373 \tLoss: 1.4680588245391846\nEpoch: 4 \tBatch: 374 \tLoss: 1.4634156227111816\nEpoch: 4 \tBatch: 375 \tLoss: 1.4686609506607056\nEpoch: 4 \tBatch: 376 \tLoss: 1.4618980884552002\nEpoch: 4 \tBatch: 377 \tLoss: 1.4624391794204712\nEpoch: 4 \tBatch: 378 \tLoss: 1.4846221208572388\nEpoch: 4 \tBatch: 379 \tLoss: 1.4628283977508545\nEpoch: 4 \tBatch: 380 \tLoss: 1.4708530902862549\nEpoch: 4 \tBatch: 381 \tLoss: 1.4826189279556274\nEpoch: 4 \tBatch: 382 \tLoss: 1.4671146869659424\nEpoch: 4 \tBatch: 383 \tLoss: 1.4685112237930298\nEpoch: 4 \tBatch: 384 \tLoss: 1.4716145992279053\nEpoch: 4 \tBatch: 385 \tLoss: 1.461151123046875\nEpoch: 4 \tBatch: 386 \tLoss: 1.4787960052490234\nEpoch: 4 \tBatch: 387 \tLoss: 1.4612032175064087\nEpoch: 4 \tBatch: 388 \tLoss: 1.4615247249603271\nEpoch: 4 \tBatch: 389 \tLoss: 1.4743282794952393\nEpoch: 4 \tBatch: 390 \tLoss: 1.469662070274353\nTraining Complete. Final loss = 1.469662070274353\nTrain Accuracy: 98.956\nValidation Accuracy: 99.03\nTest Accuracy: 98.42\n"
    }
   ],
   "source": [
    "#Try a larger learning rate\n",
    "cnn_with_bn = ConvNet(batch_norm=True)\n",
    "optimiser = torch.optim.Adam(cnn_with_bn.parameters(), lr=0.05) #before lr=0.0005\n",
    "train(cnn_with_bn)\n",
    "print('Train Accuracy:', calc_accuracy(cnn_with_bn, train_loader))\n",
    "print('Validation Accuracy:', calc_accuracy(cnn_with_bn, val_loader))\n",
    "print('Test Accuracy:', calc_accuracy(cnn_with_bn, test_loader))"
   ]
  },
  {
   "source": [
    "![Loss Comparison](images/loss.png)\n",
    "From the three runs, we can see that the vanilla CNN (orange) took the longest to converge. The CNN with batch normalisation (red) was much faster to do so and we were even able to use a larger learning rate (blue), which leads to even faster convergence, because we have a more robust cnn due to batch normalisation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.0001, 0.001, 0.01, 0.1]\n",
    "models = [\n",
    "    {\n",
    "        'name': 'with batch normalisation',\n",
    "        'model_class': CNN\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "With batch normalisation, we can use larger learning rates and have to be less careful with the selection of initial parameters, which leads to faster convergence and better performance. The networks do not even need to be trained so long, hence we could introduce early stopping now. If you want to learn about early stopping, you can do so [here](https://www.kaggle.com/akhileshrai/tutorial-early-stopping-vanilla-rnn-pytorch). Batch normalisation even works as a regularisation technique. By estimating the mean and variance for each batch, we add noise to the input data of each following layer, which helps the regularisation capability of the network. <br>\n",
    "It should be used with caution in combination with small batch sizes as the estimation of the mean and variance per batch is unstable, if we only have a small batch sample to calculate these values from.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('main': conda)",
   "display_name": "Python 3.8.5 64-bit ('main': conda)",
   "metadata": {
    "interpreter": {
     "hash": "06c1e258a470a687113bfba03f207c092b27379067ada2d83b8b31269ab641fe"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}