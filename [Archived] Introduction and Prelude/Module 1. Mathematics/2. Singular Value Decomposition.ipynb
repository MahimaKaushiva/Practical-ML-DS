{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition (SVD)\n",
    "\n",
    "### Learning Objectives:\n",
    "- [Gram Schmidt Orthonormalization Process](#Gram-Schmidt-Orthonormalization-Process)\n",
    "- [SVD: Introduction](#SVD:-Introduction)\n",
    "- [Reduced SVD](#Reduced-SVD)\n",
    "\n",
    "\n",
    "# Gram Schmidt Orthonormalization Process\n",
    "__Orthogonalization__ is the process where, given a set of vectors, we output a corresponding orthogonal set of vectors. When we want the output vectors to have unit length, the process is instead called __orthonormalization__. The __Gram-Schmidt Orthonormalization Process__ is one of the methods through which we can orthogonalize a set of vectors. It is an important concept that we will meet again once we deal with singular value decomposition.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/e/ee/Gram-Schmidt_orthonormalization_process.gif\"\n",
    "     alt=\"Orthonormalization\"/>\n",
    "\n",
    "The Gram-Schimdt Process begins by normalizing a first vector and continuously rewriting the remaining vectors in terms of themselves minus their inner-product with the already normalized vectors. We carry this process out __recursively__ until all vectors are orthonormal with respect to each other! If we have a set of $K$ vectors: $\\{\\mathbf{\\vec{v_{1}},...,\\vec{v_{k}},...,\\vec{v_{K}}}\\}$, and want to obtain a set of orthonormal vectors: $\\{\\mathbf{\\hat{e_{1}},...,\\hat{e_{k}},...,\\hat{e_{K}}}\\}$, we can generalize the process as follows:\n",
    "\n",
    "$$\\mathbf{\\vec{u_{1}}} = \\mathbf{\\vec{v_{1}}}, \\;\\;\\; \n",
    "\\mathbf{\\hat{e_{1}}} = \\frac{\\mathbf{\\vec{u_{1}}}}{||\\mathbf{\\vec{u_{1}}}||}$$\n",
    "\n",
    "$$\\mathbf{\\vec{u_{2}}} = \\mathbf{\\vec{v_{2}}} - (\\mathbf{\\vec{v_{2}}}\\cdot \\mathbf{\\hat{e_{1}}})\\mathbf{\\hat{e_{1}}}, \\;\\;\\; \\mathbf{\\hat{e_{2}}} = \\frac{\\mathbf{\\vec{u_{2}}}}{||\\mathbf{\\vec{u_{2}}}||}$$\n",
    "\n",
    "$$\\mathbf{\\vec{u_{3}}} = \\mathbf{\\vec{v_{3}}} - (\\mathbf{\\vec{v_{3}}}\\cdot \\mathbf{\\hat{e_{1}}})\\mathbf{\\hat{e_{1}}} - (\\mathbf{\\vec{v_{3}}}\\cdot \\mathbf{\\hat{e_{2}}})\\mathbf{\\hat{e_{2}}}, \\;\\;\\;\n",
    "\\mathbf{\\hat{e_{3}}} = \\frac{\\mathbf{\\vec{u_{3}}}}{||\\mathbf{\\vec{u_{3}}}||}$$\n",
    "\n",
    "$$ \\vdots$$\n",
    "\n",
    "$$ \\mathbf{\\vec{u_{k}}} = \\mathbf{\\vec{v_{k}}} - \\sum_{i=1}^{k-1}\\mathbf{\\vec{v_{k}}\\cdot \\hat{e_{i}}}, \\;\\;\\; \n",
    "\\mathbf{\\hat{e_{k}}} = \\frac{\\mathbf{\\vec{u_{k}}}}{||\\mathbf{\\vec{u_{k}}}||}$$\n",
    "\n",
    "$$ \\vdots$$\n",
    "\n",
    "$$ \\mathbf{\\vec{u_{K}}} = \\mathbf{\\vec{v_{K}}} - \\sum_{i=1}^{K-1}\\mathbf{\\vec{v_{K}}\\cdot \\hat{e_{i}}}, \\;\\;\\; \n",
    "\\mathbf{\\hat{e_{K}}} = \\frac{\\mathbf{\\vec{u_{K}}}}{||\\mathbf{\\vec{u_{K}}}||}$$\n",
    "\n",
    "This is a tricky concept, especially when looking at the general case, so we show an example below to make it less daunting:\n",
    "$$\\mathbf{V} = \\left( \\begin{bmatrix} 1 \\\\ 2 \\\\ 2 \\end{bmatrix}, \\;\n",
    "\\begin{bmatrix} 2 \\\\ 1 \\\\ 2 \\end{bmatrix}, \\;\n",
    "\\begin{bmatrix} 2 \\\\ 2 \\\\ 1 \\end{bmatrix} \\right)$$\n",
    "\n",
    "We first normalize the first vector:\n",
    "$$\\mathbf{\\hat{e_{1}}} = \\frac{\\mathbf{\\vec{v_{1}}}}{||\\mathbf{\\vec{v_{1}}}||} = \n",
    "\\frac{1}{3}\\begin{bmatrix} 1 \\\\ 2 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 0.33 \\\\ 0.67 \\\\ 0.67 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We then find the orthonormalized substitute for $\\mathbf{\\vec{v_{2}}}$ by subtracting its projection onto $\\mathbf{\\hat{e_{1}}}$:\n",
    "\n",
    "$$\\mathbf{\\vec{u_{2}}} = \\mathbf{\\vec{v_{2}}} - (\\mathbf{\\vec{v_{2}}} \\cdot \\mathbf{\\hat{e_{1}}})\\mathbf{\\hat{e_{1}}}=\n",
    "\\begin{bmatrix} 2 \\\\ 1 \\\\ 2 \\end{bmatrix} -\\left( \\begin{bmatrix} 2 \\\\ 1 \\\\ 2 \\end{bmatrix}\\cdot \\frac{1}{3}\\begin{bmatrix} 1 \\\\ 2 \\\\ 2 \\end{bmatrix}\\right) \\frac{1}{3}\\begin{bmatrix} 1 \\\\ 2 \\\\ 2 \\end{bmatrix} =\n",
    "\\begin{bmatrix} 2 \\\\ 1 \\\\ 2 \\end{bmatrix} - \\frac{8}{9}\\begin{bmatrix} 1 \\\\ 2 \\\\ 2 \\end{bmatrix} = \n",
    "\\frac{1}{9}\\begin{bmatrix} 10 \\\\ -7 \\\\ 2 \\end{bmatrix}\n",
    "$$\n",
    "$$\\mathbf{\\hat{e_{2}}} = \\frac{\\mathbf{\\vec{u_{2}}}}{||\\mathbf{\\vec{u_{2}}}||} = \n",
    "\\frac{1}{\\sqrt{153}}\\begin{bmatrix} 10 \\\\ -7 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 0.81 \\\\ -0.57 \\\\ 0.16 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Finally, we can subtract both projections from our 3rd vector, $\\mathbf{\\vec{v_{3}}}$, and normalizing it to obtain its orthonormalized substitute:\n",
    "\n",
    "$$\\mathbf{\\vec{u_{3}}} = \\mathbf{\\vec{v_{3}}} - (\\mathbf{\\vec{v_{3}}} \\cdot \\mathbf{\\hat{e_{1}}})\\mathbf{\\hat{e_{1}}}\n",
    "-(\\mathbf{\\vec{v_{3}}} \\cdot \\mathbf{\\hat{e_{2}}})\\mathbf{\\hat{e_{2}}}=\n",
    "\\begin{bmatrix} 2 \\\\ 2 \\\\ 1 \\end{bmatrix} -\\left( \\begin{bmatrix} 2 \\\\ 2 \\\\ 1 \\end{bmatrix}\\cdot \\frac{1}{3}\\begin{bmatrix} 1 \\\\ 2 \\\\ 2 \\end{bmatrix}\\right) \\frac{1}{3}\\begin{bmatrix} 1 \\\\ 2 \\\\ 2 \\end{bmatrix} - \n",
    "\\left( \\begin{bmatrix} 2 \\\\ 2 \\\\ 1 \\end{bmatrix}\\cdot \\frac{1}{\\sqrt{153}}\\begin{bmatrix} 10 \\\\ -7 \\\\ 2 \\end{bmatrix}\\right) \\frac{1}{\\sqrt{153}}\\begin{bmatrix} 10 \\\\ -7 \\\\ 2 \\end{bmatrix}=$$\n",
    "\n",
    "$$=\\begin{bmatrix} 2 \\\\ 2 \\\\ 1 \\end{bmatrix} - \\frac{8}{9}\\begin{bmatrix} 1 \\\\ 2 \\\\ 2 \\end{bmatrix}  -\n",
    "\\frac{8}{153}\\begin{bmatrix} 10 \\\\ -7 \\\\ 2 \\end{bmatrix} = \n",
    "\\begin{bmatrix} 0.59 \\\\ 0.59  \\\\ -0.88 \\end{bmatrix}$$\n",
    "\n",
    "$$\\mathbf{\\hat{e_{3}}} =  \\frac{\\mathbf{\\vec{u_{3}}}}{||\\mathbf{\\vec{u_{3}}}||} = \n",
    "\\begin{bmatrix} 0.49 \\\\ 0.49 \\\\ -0.73 \\end{bmatrix}$$\n",
    "\n",
    "And there you go, we have found the corresponding orthonormal set of vectors! While it is good to have some intuition behind how this process works, we will generally write programs that can accomplish this task for us to save time, and to handle much larger vector sets. Below we show you how to compute the Gram-Schmidt Orthonormalization Process from scratch using a recursive function, as well as the in-built NumPy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODING CHALLENGE: Writing our own recursive orthonormalization function!\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def gs_ortho(vector_list, column_idx=0): # takes as input the list of column vectors\n",
    "    n_rows, n_columns = np.shape(vector_list)\n",
    "    \n",
    "    # Base case:\n",
    "    if column_idx == n_columns:\n",
    "        return ## After all column vectors have been orthonormalized, what do I return?\n",
    "    \n",
    "    # Recursion call: # use loop to create a projection list, then subtract by the sum of its components\n",
    "    else:\n",
    "        current_v = vector_list[:, column_idx]##\n",
    "        projection_list = []\n",
    "        for i in range(column_idx): # used to create a list of vector projections\n",
    "            e_i = ## How do I take the e vector from our vector list?\n",
    "            inner = ##\n",
    "            projection = ## How do we get the projection from the inner product and unit vector\n",
    "            projection_list.append(projection) ##\n",
    "        \n",
    "        projection_list = np.array(projection_list).T\n",
    "        current_u = ## How do we get our vector u from our vector v and its projections?\n",
    "        current_e = ## How do we get our vector e from our vector u?\n",
    "        \n",
    "        vector_list[:, column_idx] = current_e \n",
    "        \n",
    "        return gs_ortho(vector_list, column_idx=)## What should be the column index in the next recursion call?\n",
    "    \n",
    "A = np.array([[1, 2, 2], [2, 1, 2], [2, 2, 1]], dtype=\"float_\")\n",
    "print(\"Q =\" ,gs_ortho(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the Gram-Schmidt Orthonormalization Process below via the NumPy function _np.linalg.qr( )_. 'qr' stands for QR Factorization, where we decompose a set of vectors into an orthogonal matrix(Q) and an upper triangular matrix(R). Our orthonormalized vector list is the first element of tuple the function returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our matrix\n",
    "A = np.array([[1,2,2],[2,1,2],[2,2,1]])\n",
    "\n",
    "# Applying Orthonormalization (first output of the following function)\n",
    "Q,_ = np.linalg.qr(A)\n",
    "print(\"Q =\", Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# SVD: Introduction\n",
    "\n",
    "The __singular value decomposition__ (SVD) is a highly relevant method in data science and machine learning, and is where we can bring together all we had learned in previous sections about vectors and matrices. SVD is a method that allows us to decompose a rectangular matrix into a different form. It is tailored to a given problem, as it depends on the data inside a matrix for each instance. Given that we can represent the data in a new form, it is useful for data reduction, and even the basis of __principal component analysis (PCA)__, which we will cover in a later section.\n",
    "\n",
    "SVD is regarded as one of the most important techniques in linear algebra. It is used in the Google rank algorithm to find the best matches for your search, recommender systems like Netflix and Amazon, and even facial recognition with a SVD representation fo human faces. It is also scalable to any size data set, hence why even Google can do it!!\n",
    "\n",
    "We can apply this method to any rectangular matrix. If we consider a MxN rectangular matrix, A, SVD allows us to decompose it in the following form:\n",
    "\n",
    "$$A = U\\Sigma V^{T} = \n",
    "\\begin{bmatrix}\n",
    "| & | &   & | \\\\ \\mathbf{\\vec{u_{1}}} & \\mathbf{\\vec{u_{2}}} & ... & \\mathbf{\\vec{u_{N}}} \\\\ | & | &   & | \\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\sigma_{1} & 0 & ... & 0 \\\\ 0 & \\sigma_{2} &  & 0 \\\\ \\vdots &  & \\ddots & \\vdots \\\\ 0 & ... & ... & \\sigma_{M} \\\\ -- & -- & -- & -- \\\\ 0 & 0 & ... & 0 \\\\ \\vdots & \\vdots &  & \\vdots \\\\ 0 & 0 & ... & 0 \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "| & | &   & | \\\\ \\mathbf{\\vec{v_{1}}} & \\mathbf{\\vec{v_{2}}} & ... & \\mathbf{\\vec{v_{N}}} \\\\ | & | &   & | \n",
    "\\end{bmatrix} ^{T}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- __U__ is a NxN square, orthogonal matrix, known as the __right singular vectors__\n",
    "- __V__ is a MxM square, orthogonal matrix, known as the __left singular vectors__\n",
    "- $\\mathbf{\\Sigma}$ is a MxN diagonal matrix, containing the __singular values__ along its diagonal\n",
    "\n",
    "These three matrices are unique for any given rectangular matrix (except by flipping the sign of each vector). SVD is a way of representing a matrix in a way that similar things become more similar and different things become more different, measuring the variability in different dimensions of a matrix. So how do we carry out SVD? This is where our understanding of eigenvalues and eigenvectors comes in, where construct each of the three matrices individually:\n",
    "- U: composed of the the orthonormalized eigenvectors of $AA^{T}$\n",
    "- V: composed of the orthonormalized eigenvectors of $A^{T}A$\n",
    "- $\\Sigma$: composed of the square root of the corresponding eigenvalues of $AA^{T}$ or $A^{T}A$\n",
    "\n",
    "An interesting property is that the eigenvalues of $AA^{T}$ and $A^{T}A$ are always the same so we can pick either one of them. Let us now compute the SVD of the simple matrix below. We will first use NumPy to carry out the invidiual steps, then confirm our answers with the in-built SVD function. \n",
    "\n",
    "$$A = \\begin{bmatrix} 1 & 3 & 2 \\\\ 2 & 2 & 1 \\end{bmatrix} $$\n",
    "\n",
    "We will start by determining the matrix U. In NumPy, the _numpy.linalg.eig(  )_ function returns a tuple two elements:  eigenvalues and eigvectors. However, it does not return the eigenvalues of a square matrix in the appropriate order. To account for this, we have created the function below to re-order our eigenvalues in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_eig_vectors(eig_tuple):\n",
    "    eig_values, eig_vectors = eig_tuple\n",
    "    idx = eig_values.argsort()[::-1]\n",
    "    eig_values = eig_values[idx]\n",
    "    eig_vectors = eig_vectors[:, idx]\n",
    "    return eig_values, eig_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use NumPy to compute $AA^{T}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our original matrix\n",
    "A = np.array([[1, 3, 2],\n",
    "              [2 , 2, 1]])\n",
    "\n",
    "def preU_matrix(A):\n",
    "    return np.matmul(A, A.T)\n",
    "\n",
    "AA_T = preU_matrix(A)\n",
    "print(\"AA_T:\")\n",
    "print(AA_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know that:\n",
    "$$ AA^{T} = \\begin{bmatrix} 14 & 10 \\\\ 10 & 9 \\end{bmatrix}$$\n",
    "\n",
    "We can compute its eigenvectors and eigenvalues using NumPy. We then need to carry out orthonormalization to make all the columns of our matrix orthonormal with respect to each other. We will use the previously encountered Gram-Schmidt orthonormalization process, as shown below. We incorporate all necessary steps in the _U\\_matrix( )_ function, which takes in rectangular matrix and returns the orthonormalized eigenvectors and eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def U_matrix(A):\n",
    "    AA_T = preU_matrix(A)\n",
    "    U_ = np.linalg.eig(AA_T)\n",
    "    U_eigvalues, U_eigvectors = reorder_eig_vectors(U_)\n",
    "    U,_ = np.linalg.qr(U_eigvectors)\n",
    "    return U_eigvalues, U\n",
    "\n",
    "U_eigvalues, U = U_matrix(A)\n",
    "print(\"Eigenvalues:\", U_eigvalues)\n",
    "print(\"U:\")\n",
    "print(U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our first matrix, U:\n",
    "$$U = \\begin{bmatrix} -0.788 & -0.615 \\\\ -0.615 & 0.788 \\end{bmatrix}$$\n",
    "\n",
    "The same procedure can be applied to determine the matrix, $V^{T}$, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODING CHALLENGE:\n",
    "\n",
    "# Computing the augmented matrix\n",
    "def preV_matrix(A):\n",
    "    ##\n",
    "    \n",
    "# Computing V matrix\n",
    "def V_matrix(A):\n",
    "    ##\n",
    "\n",
    "V_eigvalues, V = V_matrix(A)\n",
    "V_T = V.T\n",
    "print(\"Eigenvalues:\", V_eigvalues)\n",
    "print(\"V:\")\n",
    "print(V)\n",
    "print(\"V_T\")\n",
    "print(V_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now construct $V^{T}$ from the above eigenvectors, and $\\Sigma$ from the above eigenvalues:\n",
    "$$ V = \\begin{bmatrix} -0.432 & 0.880 & 0.196 \\\\ -0.769 & -0.247 & -0.588 \\\\ -0.469 & -0.405 & 0.784 \\end{bmatrix}, \\;\n",
    "V^{T} = \\begin{bmatrix} -0.432 & -0.769 & -0.469 \\\\ 0.880 & -0.247 & -0.405 \\\\ 0.196 & -0.588 & 0.784 \\end{bmatrix}$$\n",
    "$$ \\Sigma = diag(\\lambda_{1},...,\\lambda_{k}) = \\begin{bmatrix} \\sqrt{21.808} & 0 & 0 \\\\0 & \\sqrt{1.192} & 0 \\end{bmatrix} = \n",
    "\\begin{bmatrix} 4.670 & 0 & 0 \\\\0 & 1.092 & 0 \\end{bmatrix}$$\n",
    "\n",
    "The function that we use to construct the sigma matrix is shown below. We can even confirm our result, as the matrix product of the three matrices we have constructed should simply return A, and the in-built SVD function should return the same three matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing our Sigma matrix\n",
    "def S_matrix(A):\n",
    "    rows, columns = np.shape(A)\n",
    "    U_ = U_matrix(A)\n",
    "    \n",
    "    # Getting only appropriate number of eigenvalues in correct order\n",
    "    eigvalues,_ = reorder_eig_vectors(U_)\n",
    "    min_dim = min(rows, columns)\n",
    "    eigvalues = eigvalues[0:min_dim]\n",
    "    \n",
    "    S = np.zeros((rows,columns))\n",
    "    np.fill_diagonal(S,np.sqrt(eigvalues))\n",
    "    return S\n",
    "\n",
    "S = S_matrix(A)\n",
    "print(\"Sigma:\")\n",
    "print(S)\n",
    "print()\n",
    "\n",
    "# Checking whether we get the original matrix (Numpy function allclose to check if it is the same as the original)\n",
    "A_check = np.matmul(U, np.matmul(S, V_T))\n",
    "print(\"A_reconstruct:\")\n",
    "print(A_check)\n",
    "print()\n",
    "print(np.allclose(A, A_check))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now incorporate our individual functions into our own SVD function and compare it to the in-built NumPy SVD function. Note that in this case, we are able to successfully compute the SVD for A. However, due to some limitations of the _np.linalg.eig(  )_, function, we may get an SVD that permutes the rows of our reconstructed matrix. Therefore, while a good exercise, we always recommend using the in-built SVD function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODING CHALLENGE:\n",
    "\n",
    "def svd(A):\n",
    "    ##\n",
    "\n",
    "U, S, V_T = svd(A)\n",
    "print(\"U:\")\n",
    "print(U)\n",
    "print(\"Sigma:\")\n",
    "print(S)\n",
    "print(\"V_T :\")\n",
    "print(V_T)\n",
    "print()\n",
    "U, S, V_T = np.linalg.svd(A)\n",
    "print(\"U:\")\n",
    "print(U)\n",
    "print(\"Sigma:\")\n",
    "print(S)\n",
    "print(\"V_T:\")\n",
    "print(V_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Reduced SVD\n",
    "\n",
    "\n",
    "This decomposition contains all the pieces that we need to put our original matrix back together. The singular values($\\sigma_{i}$) are arranged in a hierarchical way such that:\n",
    "$$\\sigma_{1} > \\sigma_{2} > ... > \\sigma_{M}$$\n",
    "\n",
    "Since diagonal matrices scale each row of a matrix by the respective diagonal component, when we carry out SVD, __these singular values represent the importance of the vectors in U and V__. Given this structure, the vectors in U and V are also arranged according to their importance. This allows us to carry out what is known as the __reduced singular value decomposition__. By only keeping the first R components of the SVD representation, where R < M, R < N, given that R is large enough, we can obtain an __approximation__ of the original matrix, $\\hat{A}$, that is good enough given our application of SVD with a smaller number of coefficients. This is the reason why SVD is such a powerful method in dimensionality reduction, and allows us to only keep as much variability of the matrix as required for inputing data into a model for example.\n",
    "\n",
    "An example of dimensionality reduction below is in image compression, where we can use SVD to compress an image into a much smaller amount of information. As shown below, the more components we keep from the original SVD, the better our approximation will be. In practice, we may be content with an imperfect approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import rgb2gray\n",
    "\n",
    "# Uses SVD to construct matrix approximations\n",
    "def reduced_svd(A, R):\n",
    "    U, S, V_T = np.linalg.svd(A)\n",
    "    \n",
    "    # Obtaining approximation of matrices with R components\n",
    "    U_hat = U[:,0:R]\n",
    "    S_hat = S[0:R]\n",
    "    V_T_hat = V_T[0:R,:]\n",
    "    return U_hat, S_hat, V_T_hat\n",
    "\n",
    "# Computes apprimation of original matrix\n",
    "def approx(A, R):\n",
    "    U_hat, S_hat, V_T_hat = reduced_svd(A, R)\n",
    "    S_hat_matrix = np.zeros((S_hat.shape[0], S_hat.shape[0]))\n",
    "    np.fill_diagonal(S_hat_matrix, S_hat)\n",
    "\n",
    "    return np.matmul(U_hat, np.matmul(S_hat_matrix, V_T_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importing our image\n",
    "img = plt.imread(\"images/svd_skeleton.jpg\")\n",
    "img = rgb2gray(img)\n",
    "\n",
    "# Determining parameters\n",
    "R_vals = [5, 10, 50, 100, 500]\n",
    "\n",
    "# Displaying approximation\n",
    "for R in R_vals:\n",
    "    new_approx = approx(img, R)\n",
    "    print(\"R =\", R)\n",
    "    plt.figure()\n",
    "    plt.imshow(new_approx, cmap='gray')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
