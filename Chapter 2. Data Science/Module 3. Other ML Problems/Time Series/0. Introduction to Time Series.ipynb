{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Time Series\n",
    "\n",
    "## Learning Objectives\n",
    "- What is time series data?\n",
    "- What is forecasting?\n",
    "- Evaluating forecasting\n",
    "- Simple forecasting methods\n",
    "  - Naive method\n",
    "  - Average method\n",
    "  - Moving average\n",
    "  - Weighted moving average\n",
    "- Trend, Random, Seasonality and Stationary data\n",
    "  - Dickey-Fullerâ€™s test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "A time series is a sample of measurements of some interesting quantity taken repeatedly over a sustained period of time. It is mathematically represented as a set of vectors \n",
    "$$x(t) \\ \\ \\ \\forall \\ t = 0,1,2,..,n$$\n",
    "where $t$ represents the time elapse and the variable $x(t)$ is treated as a random variable. A defining characteristic of a time series is that it is a list of observations where the ordering matters so the data is not necessarily independent and identically distributed. Changing the order of the datapoints could change the meaning of the data. Time series appear in many different disciplines in the real world such as Economics, Epidemiology, Social Sciences, Physical Sciences etc.\n",
    "\n",
    "\n",
    "Predicting the future is a challenging problem, especially if we have limited knowledge about a given phenomenon. However, we can still learn from past patterns to make an educated projection of the future.\n",
    "Timeseries are observational data of a given metric across time (e.g., daily change in temperature during a month). Analysis of these datasets allows us to disentangle regularities in historical observations (e.g., the temperature rises in summer and decreases in winter) and use them to make an informed decision about the future's potential changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# Import relevant data\n",
    "aicore_df = pd.read_csv('../../DATA/aicorestudents.csv')\n",
    "\n",
    "# Preview of the data\n",
    "total_rows = aicore_df.shape[0]\n",
    "aicore_df.head(n=total_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of the toy time series (make sure you've run previous cell)\n",
    "fig = px.line(aicore_df, x = 'Month', y = 'Students', title='Number of students at AI Core')\n",
    "fig.update_traces(mode='markers+lines')\n",
    "fig.update_layout(title={'text': \"Number of students at AI Core\",'y':0.9,'x':0.5,'xanchor': 'center','yanchor': 'top'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Forecasting?\n",
    "This notebook will be focusing on introducing time series forecasting - starting from naive methodologies and building up to the baseline of knowledge needed for some more sophisticated methods (such as ARMA/ARIMA and Holt-Winters). But first, what is the difference between the terms __time series analysis__ and __time series forecasting__? \n",
    "\n",
    "__Time series analysis__ is a form of descriptive modelling, this means that someone conducting time series analysis will be looking at a dataset to identify trends and seasonal patterns in the historical data, fitting mathematical models to capture the underlying nature of the process generating the data etc. __Time series forecasting__ is a form of predictive modelling with the goal to predict a future value at a particular point in time based on the values we do know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting Accuracy\n",
    "In a time series the values we do know are referred to as __observed values__ and the values we are trying to forecast are referred to as __expected values__. In general, we use the notation $\\mathbf{\\hat{y}}$ to denote expected values. \n",
    "\n",
    "For example, if we have a series that looks like [2,4,6,8,10], we might forecast the next value of this series to be 12. Using this terminology and notation, the observed values are $y_1=2$, $y_2=4$, $y_3=6$, $y_4=8$, $y_5=10$ i.e the observed series is [2,4,6,8,10] and the next expected value is $\\hat{y_6}=12$.\n",
    "\n",
    "It's important to have some metrics to evaluate the __accuracy__ of our forecasts. \n",
    "\n",
    "The __error__ is the difference between an observed value and its forecast. Given a training dataset $\\mathbf{\\{y_{1},\\dots, y_{T}\\}}$ and a test dataset $\\mathbf{\\{y_{T+1}, y_{T+2},\\dots\\}}$, the error of a forecast at a given time index __T+h__ is denoted as $\\mathbf{e_{T+h}=y_{T+h} - \\hat{y_{T+h}}}$. \n",
    "\n",
    "As the error can be positive or negative it is more helpful to use the absolute terms or as common convention square the error so the value is always positive. \n",
    "\n",
    "The __sum of squared errors (SSE)__ is given by $\\mathbf{SSE = \\sum_{i=1}^{i=n} ( y_{i} - \\hat{y_{i}})^{2}}$. The SSE measures the inexplained variability or discrepancy between the observed data and the forecasted data. Another common metric used is the __mean squared error (MSE)__ which is given by $\\mathbf{MSE=\\frac{1}{n}\\sum_{i=1}^{i=n} ( y_{i} - \\hat{y_{i}})^{2}}$. Other metrics that can also be used to assess the accuracy of a forecast are the __mean absolute error (MAE)__ given by $\\mathbf{MAE=\\frac{1}{n}\\sum_{i=1}^{i=n}|y_{i}-\\hat{y_{i}}|}$ and the __root mean squared error (RMSE)__ given by $\\mathbf{RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{i=n} (y_{i} -\\hat{y_{i}})^2}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Forecasting methods\n",
    "\n",
    "For the next couple of examples let's consider a simple (toy) time series of the number of AI Core students has every month. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy of the AI Core Time Series as an array\n",
    "aicore_series = aicore_df[\"Students\"].to_list()\n",
    "aicore_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our training set we'll be using all the the values in the series apart from the final one i.e Training set = Training Set = [0,50,200,160,240,210,200,205]. We'll be testing our forecast against the final value of the series  i.e Test Set = 230 to see how well each forecasting method works against what we see in reality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Method\n",
    "This is the simplest forecasting method. For naive forecasts we set all forecasts to be the same value as the last observed value. That is \n",
    "\n",
    "$$\\mathbf{\\hat{y}_{T+1}=y_{T}}$$\n",
    "\n",
    "For example, if we have a time series that looks like [14,20,18,17,24], then using the naive method the forecast for the next point would be 24. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK: Code up the naive method to forecast the next point in the time series\n",
    "def naive_method(series):\n",
    "    ## Insert code here\n",
    "\n",
    "# Test your function returns the correct answer using the AI Core series\n",
    "naive_forecast=naive_method(aicore_series[:-1])\n",
    "print('Your implementation of the naive method, forecast =', naive_forecast)\n",
    "print('Observed value =',aicore_series[-1])\n",
    "print('Error (abs) of forecast=',abs(naive_forecast - aicore_series[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Method\n",
    "This method is simply, the expected value of the next datapoint is the arithmetic mean of all of the previous datapoints. That is\n",
    "\n",
    "$$\\mathbf{\\hat{y}_{T+1}=\\frac{1}{T}\\sum_{i=1}^{i=T} y_{i}}$$\n",
    "\n",
    "For example, if we have a time series that looks like [19.2,17.8,15.1,14.3,15.0,16.7,15.2], then using the average method the forecast for the next point would be 16.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK: Code up the average method to forecast the next point in the time series\n",
    "def average_method(series):\n",
    "    ## Insert code here\n",
    "    \n",
    "    \n",
    "# Test your function returns the correct answer using the AI Core series\n",
    "avg_forecast=average_method(aicore_series[:-1])\n",
    "print('Your implementation of the average method, forecast =', avg_forecast)\n",
    "print('Observed value =',aicore_series[-1])\n",
    "print('Error (abs) of forecast=',abs(avg_forecast - aicore_series[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Moving Averages\n",
    "\n",
    "Instead of taking average of all points an improvement to are forecasting method is to only take the average of the n latest datapoints. In this method only the most recent values matter. In practise, this forecasting method can be effective if the right choice of n is used. \n",
    "\n",
    "$$\\mathbf{\\hat{y}_{T+1}=\\frac{1}{n}\\sum_{i=0}^{i=n-1} y_{T-i}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK: Code up the moving average method to forecast the next point in the time series\n",
    "def movingaverage_method(series,n):\n",
    "    ## Insert code here\n",
    "    \n",
    "    \n",
    "# Test your function returns the correct answer using the AI Core series\n",
    "movingavg_forecast=movingaverage_method(aicore_series[:-1],3)\n",
    "print('Your implementation of the moving average method, forecast =', movingavg_forecast)\n",
    "print('Observed value =',aicore_series[-1])\n",
    "print('Error (abs) of forecast=',abs(movingavg_forecast - aicore_series[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted Moving Averages\n",
    "\n",
    "Often we want something in between the extremes of taking a naive forecast where only the most recent datapoint is considered and taking an average of the historical data. A __weighted moving average__ is a moving average but within the window of n points each point is assigned a different weighting. Typically the most recent points are assigned a higher weight as these would be more relevant to the forecast being made. \n",
    "\n",
    "$$\\mathbf{\\hat{y}_{T+1}=\\sum_{i=1}^{i=n} w_{i} . y_{T+1-i}}$$\n",
    "\n",
    "where $\\mathbf{w_{1}, w_{2}, \\cdots, w_{n}}$ are weights to be assigned. \n",
    "\n",
    "__Note:__ The weights assigned must add to 1. Let's consider a mini example to see what happens if the weights don't add to one.\n",
    "\n",
    "__Example:__ Let's take the training portion of the aicore series = [0, 50, 200, 160, 240, 210, 200, 205] and take weights w = [0.9,0.6,0.4]. The weighted moving average forecast we get is given by forecast = (205)(0.9) + (200)(0.6) + (210)(0.4) = 388.5. This clearly gives a forecast which is way out of the range of what we have seen in our series previously!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK: Code up the moving average method to forecast the next point in the time series\n",
    "import numpy as np\n",
    "\n",
    "def weightedaverage_method(series, weights, n):\n",
    "    ## Insert code here (approx 4 lines)\n",
    "    # Hint: You may find it easier to work with arrays instead of lists\n",
    "    \n",
    "    \n",
    "# Test your function returns the correct answer using the AI Core series\n",
    "weighted_forecast = weightedaverage_method(aicore_series[:-1], [0.05, 0.15, 0.8], 3)\n",
    "print('Your implementation of the weighted moving average method, forecast =', weighted_forecast)\n",
    "print('Observed value =',aicore_series[-1])\n",
    "print('Error (abs) of forecast=',abs(weighted_forecast - aicore_series[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How to plot this with Plotly instead?\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Plot observed series\n",
    "plt.plot(aicore_series,'ko-',label='Observed Series')\n",
    "#Plot all forecasts\n",
    "plt.plot([8,9],[aicore_series[-1],naive_forecast],'ro--', label='Naive')\n",
    "plt.plot([8,9],[aicore_series[-1],avg_forecast],'go--', label='Average')\n",
    "plt.plot([8,9],[aicore_series[-1],movingavg_forecast],'bo--', label='Moving Average')\n",
    "plt.plot([8,9],[aicore_series[-1],weighted_forecast],'co--', label='Weighted Moving Average')\n",
    "plt.legend()\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Students')\n",
    "plt.title('Number of students at AI Core')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the plot, each forecasting method provides a very different next step forecast. The best forecasting method depends on the nature of the Time Series itself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trend, Random, Seasonality and Stationary data\n",
    "\n",
    "There are different kinds of time series data... \n",
    "![](https://otexts.com/fpp2/fpp_files/figure-html/stationary-1.png)\n",
    "[Source](https://otexts.com/fpp2/stationarity.html)\n",
    "\n",
    "## Trends\n",
    "Intuitively you all know what a trend is - whether a 'global' detectable upward or downward pattern is present in the data. Looking at a small window of our data, we may see our data move in the opposite direction of the more general trend, we classify a trend as upwards or downwards over a significant amount of (time) units we decide to visualise it against. Which of the plots from the above image indicate trends in the data? \n",
    "\n",
    "## Seasonality\n",
    "An example: Every summer we expect to see a rise in the number of ice-creams sold, and we expect this value to fall during winter. Seasonal data is therefore data which has regular patterns that emerge over *seasons*. Although inspired from the 4 seasons, the definition of season in this context is looser, and typically refers to a regular period in which we can see the regular patterns of ups and downs emerge. Which of the plots from the above image indicate seasonality in the data?\n",
    "\n",
    "Note that seasonality can also be categorised as **additive** or **multiplicative**. An additive seasonality exhibits a steady pattern amplitude, whilst a multiplicative seasonality exhibits a pattern amplitude which increases or decreases with time. The figure below indicates some examples:\n",
    "\n",
    "![](https://docs.oracle.com/cd/E40248_01/epm.1112/cb_pred_user/images/graphics/seasonaltypes.gif)\n",
    "[Source](https://docs.oracle.com/cd/E40248_01/epm.1112/cb_pred_user/frameset.htm?ch03s04s01.html)\n",
    "\n",
    "## Cyclic\n",
    "Cyclical data shares a similarity with seasonal data in that it has distinguishable peaks and troughs. However, these peaks and troughs do are not influenced by the time unit. Which of the plots do you think exhibit strong cyclical properties from the (main) image above?\n",
    "\n",
    "## Random\n",
    "Random (or noisy) is as the name implies - the data usually presents itself with erratic or unsystematic fluctuations. These are typically short in duration and non-repeating, and can occur either randomly, or due to the effects of unforseen events (such as an earthquake). Which of the plots from the above image show random data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stationarity assumption\n",
    "Regularities in data are also known as stationarity, the fact that we assume that the mechanism by which a given metric is generated is static over time. In other words, It doesn't mean that the series doesn't change over time, just that the way it changes does not itself change over time. Therefore, time series data which exhibit trends or seasonality are not stationary. However, you can have stationary data which has cyclical properties. Generally, stationary data will not exhibit any predictable patterns and the plots will show it to be roughly horizontal with constant variance (that is, constant mean and variance). Which plots do you think are stationary from the above image?\n",
    "\n",
    "Stationarity can be either **weak** (when the distribution of time-series has a constant mean and variance) or **strong** (if the time-series has the same distribution over any time interval). Since stationarity is an essential assumption, we will illustrate how we could test if our time-series check this assumption before jumping into the forecasting procedure.\n",
    "\n",
    "### (Augmented) Dickey Fuller test\n",
    "\n",
    "The **(Augmented) Dickey Fuller** (ADF) test is one of the most popular statistical tests. It can be used to determine the presence of unit root in the series,  and hence help us understand if the series is stationary or not. \n",
    "\n",
    "We start by defining a null and alternate hypothesis test:\n",
    "- Null Hypothesis: The series is non-stationary. In other words, its *unit root*, $\\phi$, = 1.\n",
    "- Alternate Hypothesis: The series is stationary. In other words, its *unit root*, $\\phi$, < 1.\n",
    "\n",
    "If the ADF test statistic is less than the critical value, we can reject the null hypothesis (i.e. the series is stationary). When the ADF test statistic is greater than the critical value, we fail to reject the null hypothesis (which means the series is not stationary). The critical value is determined by the Dickey-Fuller distribution.\n",
    "\n",
    "Here, we'll omit the exact maths/methodology behind the DF/ADF test. Watch [this video](https://www.youtube.com/watch?v=1opjnegd_hA) if you're tickled by the curiosity. Rather, we'll use the statsmodel library to show how you would actually determine the ADF test statistic, and then we'll run through an implementation which will allow us to obtain the values of the same test statistic and critical value so you intuitively understand how it works. At a high level, we're going to be fitting an OLS model to our data, where our Y variable is going to be the *difference* between the response variable at each timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "adf_stat = adfuller(aicore_series)\n",
    "adf_stat_value = adf_stat[0]\n",
    "adf_crit_value = adf_stat[4][\"5%\"]\n",
    "\n",
    "print(\"ADF Statistic: {}\".format(adf_stat_value))\n",
    "print(\"5% crit value: {}\".format(adf_crit_value))\n",
    "\n",
    "if adf_stat_value < adf_crit_value:\n",
    "    print(\"We can reject the null hypothesis (at alpha = 5%). The series is stationary\")\n",
    "else:\n",
    "    print(\"We fail to reject the null hypothesis (at alpha = 5%). The series is non-stationary\")\n",
    "\n",
    "print()\n",
    "print(\"p value: {}\".format(adf_stat[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by taking the running difference of the series:\n",
    "aicore_df[\"diffs\"] = aicore_df[\"Students\"].diff()\n",
    "aicore_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we'll create a new dataframe which will contain an X and Y variable.\n",
    "# The Y variable will be all the non-NaN diffs\n",
    "# The X variable will be all but the most recent response variable\n",
    "\n",
    "df_df = pd.DataFrame()\n",
    "df_df[\"Y\"] = aicore_df[\"diffs\"][~np.isnan(aicore_df[\"diffs\"])]\n",
    "df_df[\"X\"] = aicore_df[\"Students\"].values[:-1]\n",
    "\n",
    "df_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we fit an OLS model to our data!\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "smf.ols(\"Y ~ X\", df_df).fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ADF test statistic is then just the reported t-statistic for the X variable (-2.471). Recall from the hypothesis testing notebook that we compare the test statistic with a critical value which has been obtained from a distribution. Deriving the Dickey-Fuller distribution is non-trivial though and thus we will omit it (see [here](https://stats.stackexchange.com/a/213589) for information on how to determine it). However, we can compare it with a table of critical values:\n",
    "![](https://www.statisticshowto.com/wp-content/uploads/2016/06/df-critical.png)\n",
    "[Source](https://www.statisticshowto.com/adf-augmented-dickey-fuller-test/).\n",
    "\n",
    "Alternatively, it would be more accuarte to use the `adfuller` method to determine the critical value. But at that point, we might aswell just use the method for the whole test ðŸ˜‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
